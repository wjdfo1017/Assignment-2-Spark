{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('22', '  <row Id=\"19\" PostHistoryTypeId=\"2\" PostId=\"10\" RevisionGUID=\"f9b92c74-c38f-419e-967a-006663d28a75\" CreationDate=\"2014-05-14T00:53:43.273\" UserId=\"22\" Text=\"One book that\\'s freely available is &quot;The Elements of Statistical Learning&quot; by Hastie, Tibshirani, and Friedman (published by Springer): [see Tibshirani\\'s website][1].&#xD;&#xA;&#xD;&#xA;Another fantastic source, although it isn\\'t a book, is Andrew Ng\\'s Machine Learning course on Coursera. This has a much more applied-focus than the above book, and Prof. Ng does a great job of explaining the thinking behind several different machine learning algorithms/situations.&#xD;&#xA;&#xD;&#xA;  [1]: http://statweb.stanford.edu/~tibs/ElemStatLearn/\" />  <row Id=\"183\" PostHistoryTypeId=\"2\" PostId=\"72\" RevisionGUID=\"dfebb0af-4088-4bec-8dcb-52266bfe44b2\" CreationDate=\"2014-05-14T22:40:40.363\" UserId=\"22\" Text=\"One algorithm that can be used for this is the [k-means clustering algorithm](http://en.wikipedia.org/wiki/K-means_clustering).&#xD;&#xA;&#xD;&#xA;Basically:&#xD;&#xA;&#xD;&#xA;1. Randomly choose k datapoints from your set, m_1, ..., m_k.&#xD;&#xA;2. &quot;Until convergence&quot;:&#xD;&#xA;&#xD;&#xA;    1. Assign your data points to k clusters, where cluster i is the set of points for which m_i is the closest of your current means&#xD;&#xA;    2. Replace each m_i by the mean of all points assigned to cluster i.&#xD;&#xA;&#xD;&#xA;It is good practice to repeat this algorithm several times, then choose the outcome that minimizes distances between the points of each cluster i and the center m_i.&#xD;&#xA;&#xD;&#xA;Of course, you have to know k to start here; you can use cross-validation to choose this parameter, though.\" />  <row Id=\"365\" PostHistoryTypeId=\"6\" PostId=\"130\" RevisionGUID=\"eda67480-4c12-4dbc-9db0-5dc9d422975d\" CreationDate=\"2014-05-18T13:32:26.123\" UserId=\"22\" Comment=\"This question doesn\\'t actually involve natural language processing.\" Text=\"&lt;dimensionality-reduction&gt;&lt;feature-selection&gt;\" />'), ('66', '  <row Id=\"29\" PostHistoryTypeId=\"2\" PostId=\"14\" RevisionGUID=\"bd8a5c03-7143-4cc0-9d50-beafcdffd19a\" CreationDate=\"2014-05-14T01:25:59.677\" UserId=\"66\" Text=\"I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.&#xD;&#xA;&#xD;&#xA;My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?\" />  <row Id=\"30\" PostHistoryTypeId=\"1\" PostId=\"14\" RevisionGUID=\"bd8a5c03-7143-4cc0-9d50-beafcdffd19a\" CreationDate=\"2014-05-14T01:25:59.677\" UserId=\"66\" Text=\"Is Data Science the Same as Data Mining?\" />  <row Id=\"31\" PostHistoryTypeId=\"3\" PostId=\"14\" RevisionGUID=\"bd8a5c03-7143-4cc0-9d50-beafcdffd19a\" CreationDate=\"2014-05-14T01:25:59.677\" UserId=\"66\" Text=\"&lt;data-mining&gt;\" />  <row Id=\"77759\" PostHistoryTypeId=\"2\" PostId=\"26520\" RevisionGUID=\"fbfe7790-7107-4c83-854f-3edb25f02cfc\" CreationDate=\"2018-01-11T13:59:57.087\" UserId=\"66\" Text=\"I ask from the practitioner point-of-view, and I hope the answer does not come down to nit-picking, but I would like to settle the matter once and for all in the work I do.&#xD;&#xA;&#xD;&#xA;One of the components of an Information Extraction (IE) pipeline is a relation(ship) extractor which picks out associations between entity mentions.  [Stanford CoreNLP][1] talks about this extractor strictly using the term relation(s) extractor.  Not that it is authoritative, but [Wikipedia][2] talks about relationship extraction.  The former seems more prevalent based upon both code and papers I have come across.  So is there an authoritative answer as to which is more proper, or are there legitimate cases to use both when talking about what we are trying to do?&#xD;&#xA;&#xD;&#xA;  [1]: https://nlp.stanford.edu/software/relationExtractor.html&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Relationship_extraction\" />  <row Id=\"77760\" PostHistoryTypeId=\"1\" PostId=\"26520\" RevisionGUID=\"fbfe7790-7107-4c83-854f-3edb25f02cfc\" CreationDate=\"2018-01-11T13:59:57.087\" UserId=\"66\" Text=\"In practice, is relation extraction or relationship extraction the correct term?\" />  <row Id=\"77761\" PostHistoryTypeId=\"3\" PostId=\"26520\" RevisionGUID=\"fbfe7790-7107-4c83-854f-3edb25f02cfc\" CreationDate=\"2018-01-11T13:59:57.087\" UserId=\"66\" Text=\"&lt;nlp&gt;&lt;stanford-nlp&gt;\" />'), ('63', '  <row Id=\"36\" PostHistoryTypeId=\"2\" PostId=\"16\" RevisionGUID=\"3b75652d-4d40-448c-9242-9579560f4634\" CreationDate=\"2014-05-14T01:57:56.880\" UserId=\"63\" Text=\"I use [Libsvm][1] to training data and predict classification on **semantic analysis**.&#xD;&#xA;&#xD;&#xA;But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** issue.&#xD;&#xA;&#xD;&#xA;Last year, [Liblinear][2] was release and it can solve performance issue.&#xD;&#xA;But it cost too much **memory**. &#xD;&#xA;&#xD;&#xA;Is **MapReduce** the only way to solve semantic analysis on big data?&#xD;&#xA;&#xD;&#xA;Or have any other methods can improve memory issue on **Liblinear**.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/&#xD;&#xA;  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/\" />  <row Id=\"37\" PostHistoryTypeId=\"1\" PostId=\"16\" RevisionGUID=\"3b75652d-4d40-448c-9242-9579560f4634\" CreationDate=\"2014-05-14T01:57:56.880\" UserId=\"63\" Text=\"Use liblinear on big data\" />  <row Id=\"38\" PostHistoryTypeId=\"3\" PostId=\"16\" RevisionGUID=\"3b75652d-4d40-448c-9242-9579560f4634\" CreationDate=\"2014-05-14T01:57:56.880\" UserId=\"63\" Text=\"&lt;machine-learning&gt;&lt;bigdata&gt;&lt;libsvm&gt;\" />  <row Id=\"40\" PostHistoryTypeId=\"5\" PostId=\"16\" RevisionGUID=\"f49ef7e0-7a84-4e16-a28c-520e7948d0f4\" CreationDate=\"2014-05-14T02:04:42.460\" UserId=\"63\" Comment=\"added 12 characters in body\" Text=\"I use [Libsvm][1] to training data and predict classification on **semantic analysis** issue.&#xD;&#xA;&#xD;&#xA;But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** issue.&#xD;&#xA;&#xD;&#xA;Last year, [Liblinear][2] was release and it can solve performance issue.&#xD;&#xA;But it cost too much **memory**. &#xD;&#xA;&#xD;&#xA;Is **MapReduce** the only way to solve semantic analysis issue on big data?&#xD;&#xA;&#xD;&#xA;Or have any other methods can improve memory issue on **Liblinear**.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/&#xD;&#xA;  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/\" />  <row Id=\"43\" PostHistoryTypeId=\"5\" PostId=\"16\" RevisionGUID=\"dfabe8d8-bc68-4f23-8978-86f1d0f44aba\" CreationDate=\"2014-05-14T03:40:25.897\" UserId=\"63\" Comment=\"added 16 characters in body\" Text=\"I use [Libsvm][1] to training data and predict classification on **semantic analysis** problem.&#xD;&#xA;&#xD;&#xA;But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** problem.&#xD;&#xA;&#xD;&#xA;Last year, [Liblinear][2] was release and it can solve performance bottleneck.&#xD;&#xA;But it cost too much **memory**. &#xD;&#xA;&#xD;&#xA;Is **MapReduce** the only way to solve semantic analysis problem on big data?&#xD;&#xA;&#xD;&#xA;Or have any other methods can improve memory bottleneck on **Liblinear**.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/&#xD;&#xA;  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/\" />  <row Id=\"48\" PostHistoryTypeId=\"5\" PostId=\"16\" RevisionGUID=\"e9edc983-c391-4af9-b89d-bb37e571010d\" CreationDate=\"2014-05-14T05:15:27.797\" UserId=\"63\" Comment=\"edited body\" Text=\"I use [Libsvm][1] to training data and predict classification on **semantic analysis** problem.&#xD;&#xA;&#xD;&#xA;But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** problem.&#xD;&#xA;&#xD;&#xA;Last year, [Liblinear][2] was release and it can solve performance bottleneck.&#xD;&#xA;But it cost too much **memory**. &#xD;&#xA;&#xD;&#xA;Is **MapReduce** the only way to solve semantic analysis problem on big data?&#xD;&#xA;&#xD;&#xA;Or have any other methods can improve memory bottleneck on **Liblinear**?&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/&#xD;&#xA;  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/\" />  <row Id=\"200\" PostHistoryTypeId=\"4\" PostId=\"16\" RevisionGUID=\"c09c6eba-b8ad-452a-bce4-0efaf3f1c8d0\" CreationDate=\"2014-05-15T02:41:27.063\" UserId=\"63\" Comment=\"edited title\" Text=\"Use liblinear on big data for semantic analysis\" />  <row Id=\"274\" PostHistoryTypeId=\"5\" PostId=\"17\" RevisionGUID=\"20a8bb9a-c7d1-4f93-bd5f-d735df0df7c4\" CreationDate=\"2014-05-16T13:44:53.470\" UserId=\"63\" Comment=\"added 196 characters in body\" Text=\"[LIBSVM][1] is a library for support vector classification (SVM) and regression.&#xD;&#xA;It was created by Chih-Chung Chang and Chih-Jen Lin in 2001.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/\" />'), ('97', '  <row Id=\"53\" PostHistoryTypeId=\"2\" PostId=\"22\" RevisionGUID=\"deac1274-d6c8-4534-82f2-ce6d40be137b\" CreationDate=\"2014-05-14T05:58:21.927\" UserId=\"97\" Text=\"My set of data contains a set of numeric attributes and one categorical.&#xD;&#xA;&#xD;&#xA;Say, `NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr`, &#xD;&#xA;&#xD;&#xA;where `CategoricalAttr` takes one of three possible values: `CategoricalAttrValue1`, `CategoricalAttrValue2` or `CategoricalAttrValue3`.&#xD;&#xA;&#xD;&#xA;I\\'m using default k-means clustering algorithm implementation for Octave https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/.&#xD;&#xA;It works with numeric data only.&#xD;&#xA;&#xD;&#xA;So my question: is it correct to split the categorical attribute `CategoricalAttr` into three numeric (binary) variables, like `IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3` ?\" />  <row Id=\"54\" PostHistoryTypeId=\"1\" PostId=\"22\" RevisionGUID=\"deac1274-d6c8-4534-82f2-ce6d40be137b\" CreationDate=\"2014-05-14T05:58:21.927\" UserId=\"97\" Text=\"K-Mean clustering for mixed numeric and categorical data\" />  <row Id=\"55\" PostHistoryTypeId=\"3\" PostId=\"22\" RevisionGUID=\"deac1274-d6c8-4534-82f2-ce6d40be137b\" CreationDate=\"2014-05-14T05:58:21.927\" UserId=\"97\" Text=\"&lt;data-mining&gt;&lt;clustering&gt;&lt;octave&gt;\" />  <row Id=\"56\" PostHistoryTypeId=\"2\" PostId=\"23\" RevisionGUID=\"50b00a66-20b3-4b36-af40-c372709c61c6\" CreationDate=\"2014-05-14T06:06:13.603\" UserId=\"97\" Text=\"Data Science specialization from Johns Hopkins University at Coursera would be a great start.&#xD;&#xA;https://www.coursera.org/specialization/jhudatascience/1\" />  <row Id=\"57\" PostHistoryTypeId=\"4\" PostId=\"22\" RevisionGUID=\"a6461050-177b-4a17-b842-aaa274e039b1\" CreationDate=\"2014-05-14T06:11:22.330\" UserId=\"97\" Comment=\"Renamed \\'K-Mean\\' to \\'K-Means\\' and added tag\" Text=\"K-Means clustering for mixed numeric and categorical data\" />  <row Id=\"58\" PostHistoryTypeId=\"6\" PostId=\"22\" RevisionGUID=\"a6461050-177b-4a17-b842-aaa274e039b1\" CreationDate=\"2014-05-14T06:11:22.330\" UserId=\"97\" Comment=\"Renamed \\'K-Mean\\' to \\'K-Means\\' and added tag\" Text=\"&lt;data-mining&gt;&lt;clustering&gt;&lt;octave&gt;&lt;k-means&gt;\" />  <row Id=\"100\" PostHistoryTypeId=\"5\" PostId=\"22\" RevisionGUID=\"2322a126-81bc-49e2-ab1e-c55af2f9dfa8\" CreationDate=\"2014-05-14T11:13:12.423\" UserId=\"97\" Comment=\"Cosmetic change in text.\" Text=\"My data set contains a number of numeric attributes and one categorical.&#xD;&#xA;&#xD;&#xA;Say, `NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr`, &#xD;&#xA;&#xD;&#xA;where `CategoricalAttr` takes one of three possible values: `CategoricalAttrValue1`, `CategoricalAttrValue2` or `CategoricalAttrValue3`.&#xD;&#xA;&#xD;&#xA;I\\'m using default k-means clustering algorithm implementation for Octave https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/.&#xD;&#xA;It works with numeric data only.&#xD;&#xA;&#xD;&#xA;So my question: is it correct to split the categorical attribute `CategoricalAttr` into three numeric (binary) variables, like `IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3` ?\" />  <row Id=\"276\" PostHistoryTypeId=\"6\" PostId=\"7\" RevisionGUID=\"44b379df-3358-477b-99e6-8a9a34da74ce\" CreationDate=\"2014-05-16T13:45:00.237\" UserId=\"97\" Comment=\"Added relevant tags\" Text=\"&lt;education&gt;&lt;open-source&gt;\" />  <row Id=\"502\" PostHistoryTypeId=\"2\" PostId=\"179\" RevisionGUID=\"f28d2d21-10bf-42bf-9183-e665d6af7aa5\" CreationDate=\"2014-05-21T07:09:20.093\" UserId=\"97\" Text=\"My first guess is to [visualize social network in Tableau][1].&#xD;&#xA;&#xD;&#xA;And particularly: [building network graphs in Tableau][2].&#xD;&#xA;&#xD;&#xA;What you need is to add time dimension to the &quot;Pages&quot; section to be able to see network change dynamics.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.google.com/search?q=visualize%20social%20network%20in%20tableau&#xD;&#xA;  [2]: http://www.clearlyandsimply.com/clearly_and_simply/2012/12/build-network-graphs-in-tableau.html\" />  <row Id=\"505\" PostHistoryTypeId=\"5\" PostId=\"179\" RevisionGUID=\"4ea482c2-11b7-4c82-a47d-9af31a79e9d7\" CreationDate=\"2014-05-21T07:18:48.453\" UserId=\"97\" Comment=\"added screen shot\" Text=\"My first guess is to [visualize social network in Tableau][1].&#xD;&#xA;&#xD;&#xA;And particularly: [building network graphs in Tableau][2].&#xD;&#xA;&#xD;&#xA;What you need is to add time dimension to the &quot;Pages&quot; section to be able to see network change dynamics.&#xD;&#xA;&#xD;&#xA;This is screen from the link above.&#xD;&#xA;![enter image description here][3]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.google.com/search?q=visualize%20social%20network%20in%20tableau&#xD;&#xA;  [2]: http://www.clearlyandsimply.com/clearly_and_simply/2012/12/build-network-graphs-in-tableau.html&#xD;&#xA;  [3]: http://i.stack.imgur.com/Quq8G.png\" />  <row Id=\"550\" PostHistoryTypeId=\"2\" PostId=\"201\" RevisionGUID=\"5209e052-c2f4-442e-b31f-d6a7582b8514\" CreationDate=\"2014-05-23T09:09:44.490\" UserId=\"97\" Text=\" In addition to the listed sources: some social network data sets.&#xD;&#xA;&#xD;&#xA; - [Stanford University large network dataset collection (SNAP)][1]&#xD;&#xA; - [A huge twitter dataset that includes followers][2] + [large collection of twitter datasets here][3]&#xD;&#xA; - [LastFM data set][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://snap.stanford.edu/data/&#xD;&#xA;  [2]: http://blog.infochimps.com/2008/12/29/massive-scrape-of-twitters-friend-graph/&#xD;&#xA;  [3]: http://www.infochimps.com/collections/twitter-census&#xD;&#xA;  [4]: http://mtg.upf.edu/node/1671\" />  <row Id=\"551\" PostHistoryTypeId=\"5\" PostId=\"201\" RevisionGUID=\"139ebe29-5d52-4bde-b875-7567db44d747\" CreationDate=\"2014-05-23T09:19:41.627\" UserId=\"97\" Comment=\"added more sources\" Text=\"In addition to the listed sources.&#xD;&#xA;&#xD;&#xA;Some social network data sets:&#xD;&#xA;&#xD;&#xA; - [Stanford University large network dataset collection (SNAP)][1]&#xD;&#xA; - [A huge twitter dataset that includes followers][2] + [large collection of twitter datasets here][3]&#xD;&#xA; - [LastFM data set][4]&#xD;&#xA;&#xD;&#xA;There are plenty of sources listed at Stats SE:&#xD;&#xA;&#xD;&#xA; - [Locating freely available data samples][5]&#xD;&#xA; - [Data APIs/feeds available as packages in R][6]&#xD;&#xA; - [Free data set for very high dimensional classification][7]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://snap.stanford.edu/data/&#xD;&#xA;  [2]: http://blog.infochimps.com/2008/12/29/massive-scrape-of-twitters-friend-graph/&#xD;&#xA;  [3]: http://www.infochimps.com/collections/twitter-census&#xD;&#xA;  [4]: http://mtg.upf.edu/node/1671&#xD;&#xA;  [5]: http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/&#xD;&#xA;  [6]: http://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r&#xD;&#xA;  [7]: http://stats.stackexchange.com/questions/973/free-data-set-for-very-high-dimensional-classification\" />  <row Id=\"664\" PostHistoryTypeId=\"6\" PostId=\"22\" RevisionGUID=\"3073a77e-c4c3-4037-b200-abbcc996fa19\" CreationDate=\"2014-06-10T07:53:48.253\" UserId=\"97\" Comment=\"Added 1 more relevant tag\" Text=\"&lt;data-mining&gt;&lt;clustering&gt;&lt;octave&gt;&lt;k-means&gt;&lt;categorical-data&gt;\" />  <row Id=\"670\" PostHistoryTypeId=\"2\" PostId=\"262\" RevisionGUID=\"e14f5001-4e2e-4c4a-a094-140e20f364f1\" CreationDate=\"2014-06-10T09:26:06.593\" UserId=\"97\" Text=\"What are the main benefits from storing data in HDF? And what are the main data science tasks where HDF is really suitable and useful?\" />  <row Id=\"671\" PostHistoryTypeId=\"1\" PostId=\"262\" RevisionGUID=\"e14f5001-4e2e-4c4a-a094-140e20f364f1\" CreationDate=\"2014-06-10T09:26:06.593\" UserId=\"97\" Text=\"Benefits from using Hierarchical Data Format\" />  <row Id=\"672\" PostHistoryTypeId=\"3\" PostId=\"262\" RevisionGUID=\"e14f5001-4e2e-4c4a-a094-140e20f364f1\" CreationDate=\"2014-06-10T09:26:06.593\" UserId=\"97\" Text=\"&lt;data-formats&gt;&lt;hierarchical-data-format&gt;\" />  <row Id=\"683\" PostHistoryTypeId=\"2\" PostId=\"269\" RevisionGUID=\"b5d68d14-89a3-4759-a24c-760171f93746\" CreationDate=\"2014-06-10T11:37:28.293\" UserId=\"97\" Text=\"Definitely they can.&#xD;&#xA;I can target you to a **[nice paper][1]**. Once I used it for soccer league results prediction algorithm implementation, primarily aiming at having some value against bookmakers.&#xD;&#xA;&#xD;&#xA;From paper\\'s abstract:&#xD;&#xA;&gt; a Bayesian dynamic generalized model to estimate the time dependent skills of all teams in a league, and to predict next weekend\\'s soccer matches.&#xD;&#xA;&#xD;&#xA;Keywords:&#xD;&#xA;&#xD;&#xA;&gt; Dynamic Models, Generalized Linear Models, Graphical Models, Markov&#xD;&#xA;&gt; Chain Monte Carlo Methods, Prediction of Soccer Matches&#xD;&#xA;&#xD;&#xA;  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7448&amp;rep=rep1&amp;type=pdf\" />  <row Id=\"862\" PostHistoryTypeId=\"6\" PostId=\"356\" RevisionGUID=\"5bbe468c-237e-46a8-b52b-a95d8d6576a9\" CreationDate=\"2014-06-13T11:34:23.160\" UserId=\"97\" Comment=\"Adding relevant tags\" Text=\"&lt;scaling&gt;&lt;sql&gt;\" />  <row Id=\"923\" PostHistoryTypeId=\"2\" PostId=\"384\" RevisionGUID=\"f5a1b211-4925-470a-9ede-42735652cf94\" CreationDate=\"2014-06-15T14:01:38.233\" UserId=\"97\" Text=\"I have a binary classification problem:&#xD;&#xA;&#xD;&#xA; - Approximately 1000 samples&#xD;&#xA; - 10 attributes, including binary, numeric and categorical&#xD;&#xA;&#xD;&#xA;Which algorithm is the best choice for this type of problem?&#xD;&#xA;&#xD;&#xA;By default I\\'m going to start with SVM, as it is considered the best for relatively clean and  not noisy data. \" />  <row Id=\"924\" PostHistoryTypeId=\"1\" PostId=\"384\" RevisionGUID=\"f5a1b211-4925-470a-9ede-42735652cf94\" CreationDate=\"2014-06-15T14:01:38.233\" UserId=\"97\" Text=\"Choose binary classification algorithm\" />  <row Id=\"925\" PostHistoryTypeId=\"3\" PostId=\"384\" RevisionGUID=\"f5a1b211-4925-470a-9ede-42735652cf94\" CreationDate=\"2014-06-15T14:01:38.233\" UserId=\"97\" Text=\"&lt;classification&gt;&lt;svm&gt;\" />  <row Id=\"928\" PostHistoryTypeId=\"5\" PostId=\"384\" RevisionGUID=\"ea359b5b-dd65-4bb5-a036-0683ab6405ab\" CreationDate=\"2014-06-15T14:11:08.710\" UserId=\"97\" Comment=\"Add tag, minor changes in text\" Text=\"I have a binary classification problem:&#xD;&#xA;&#xD;&#xA; - Approximately 1000 samples in training set&#xD;&#xA; - 10 attributes, including binary, numeric and categorical&#xD;&#xA;&#xD;&#xA;Which algorithm is the best choice for this type of problem?&#xD;&#xA;&#xD;&#xA;By default I\\'m going to start with SVM, as it is considered the best for relatively clean and  not noisy data. \" />  <row Id=\"929\" PostHistoryTypeId=\"6\" PostId=\"384\" RevisionGUID=\"ea359b5b-dd65-4bb5-a036-0683ab6405ab\" CreationDate=\"2014-06-15T14:11:08.710\" UserId=\"97\" Comment=\"Add tag, minor changes in text\" Text=\"&lt;classification&gt;&lt;svm&gt;&lt;binary&gt;\" />  <row Id=\"937\" PostHistoryTypeId=\"5\" PostId=\"384\" RevisionGUID=\"7a561167-a9e3-471d-8381-84238951bb76\" CreationDate=\"2014-06-15T15:23:12.657\" UserId=\"97\" Comment=\"More details\" Text=\"I have a binary classification problem:&#xD;&#xA;&#xD;&#xA; - Approximately 1000 samples in training set&#xD;&#xA; - 10 attributes, including binary, numeric and categorical&#xD;&#xA;&#xD;&#xA;Which algorithm is the best choice for this type of problem?&#xD;&#xA;&#xD;&#xA;By default I\\'m going to start with SVM (preliminary having nominal attributes values converted to binary features), as it is considered the best for relatively clean and  not noisy data. \" />  <row Id=\"965\" PostHistoryTypeId=\"6\" PostId=\"384\" RevisionGUID=\"4c1ae7a3-e860-4ddd-b3a3-841f4038c3ba\" CreationDate=\"2014-06-16T14:02:42.467\" UserId=\"97\" Comment=\"Add tag\" Text=\"&lt;classification&gt;&lt;binary&gt;&lt;svm&gt;&lt;random-forest&gt;&lt;logistic-regression&gt;\" />  <row Id=\"1050\" PostHistoryTypeId=\"6\" PostId=\"421\" RevisionGUID=\"058b159f-e186-4b5e-996c-d62ffaaccd7e\" CreationDate=\"2014-06-17T16:17:09.043\" UserId=\"97\" Comment=\"Adding more relevant tags\" Text=\"&lt;machine-learning&gt;&lt;education&gt;&lt;beginner&gt;\" />  <row Id=\"1054\" PostHistoryTypeId=\"6\" PostId=\"403\" RevisionGUID=\"17bf7c97-819f-4f8b-8d36-438c85a013e2\" CreationDate=\"2014-06-17T16:17:36.433\" UserId=\"97\" Comment=\"Adding tag.\" Text=\"&lt;categorical-data&gt;&lt;logistic-regression&gt;\" />  <row Id=\"1257\" PostHistoryTypeId=\"2\" PostId=\"511\" RevisionGUID=\"d301b0cc-dbf6-403e-a745-1690dd130ee9\" CreationDate=\"2014-06-20T17:57:46.363\" UserId=\"97\" Text=\"I wonder which type of model cross-validation to choose: K-fold or random sub-sampling?&#xD;&#xA;&#xD;&#xA;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&#xD;&#xA;&#xD;&#xA;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&#xD;&#xA;&#xD;&#xA;On the other hand I don\\'t like random sub-sampling feature: that some items won\\'t be ever selected for training, and some will be used more than once.\" />  <row Id=\"1258\" PostHistoryTypeId=\"1\" PostId=\"511\" RevisionGUID=\"d301b0cc-dbf6-403e-a745-1690dd130ee9\" CreationDate=\"2014-06-20T17:57:46.363\" UserId=\"97\" Text=\"Cross-validation: K-fold vs Repeated random sub-sampling\" />  <row Id=\"1259\" PostHistoryTypeId=\"3\" PostId=\"511\" RevisionGUID=\"d301b0cc-dbf6-403e-a745-1690dd130ee9\" CreationDate=\"2014-06-20T17:57:46.363\" UserId=\"97\" Text=\"&lt;cross-validation&gt;&lt;sampling&gt;&lt;random-sub-sampling&gt;\" />  <row Id=\"1260\" PostHistoryTypeId=\"5\" PostId=\"511\" RevisionGUID=\"af49ddbe-c13a-480a-8f99-60b1702ce7e5\" CreationDate=\"2014-06-20T18:04:53.227\" UserId=\"97\" Comment=\"added 11 characters in body\" Text=\"I wonder which type of model cross-validation to choose: K-fold or random sub-sampling?&#xD;&#xA;&#xD;&#xA;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&#xD;&#xA;&#xD;&#xA;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&#xD;&#xA;&#xD;&#xA;On the other hand I don\\'t like random sub-sampling feature: that some items won\\'t be ever selected for training/validation, and some will be used more than once.\" />  <row Id=\"1261\" PostHistoryTypeId=\"5\" PostId=\"511\" RevisionGUID=\"dc351b36-c664-40d1-b433-a0193ce86c9b\" CreationDate=\"2014-06-20T18:33:42.860\" UserId=\"97\" Comment=\"added 72 characters in body\" Text=\"I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling?&#xD;&#xA;&#xD;&#xA;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&#xD;&#xA;&#xD;&#xA;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&#xD;&#xA;&#xD;&#xA;On the other hand I don\\'t like random sub-sampling feature: that some items won\\'t be ever selected for training/validation, and some will be used more than once.&#xD;&#xA;&#xD;&#xA;Classification algorithms used: random forest &amp; logistic regression.\" />  <row Id=\"1281\" PostHistoryTypeId=\"5\" PostId=\"511\" RevisionGUID=\"bd9459b2-c9f1-4aad-b3aa-98a034412551\" CreationDate=\"2014-06-22T10:03:50.983\" UserId=\"97\" Comment=\"Use more accurate term in header\" Text=\"I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling (bootstrap sampling)?&#xD;&#xA;&#xD;&#xA;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&#xD;&#xA;&#xD;&#xA;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&#xD;&#xA;&#xD;&#xA;On the other hand I don\\'t like random sub-sampling feature: that some items won\\'t be ever selected for training/validation, and some will be used more than once.&#xD;&#xA;&#xD;&#xA;Classification algorithms used: random forest &amp; logistic regression.\" />  <row Id=\"1282\" PostHistoryTypeId=\"4\" PostId=\"511\" RevisionGUID=\"bd9459b2-c9f1-4aad-b3aa-98a034412551\" CreationDate=\"2014-06-22T10:03:50.983\" UserId=\"97\" Comment=\"Use more accurate term in header\" Text=\"Cross-validation: K-fold vs Bootstrap sampling\" />  <row Id=\"1283\" PostHistoryTypeId=\"6\" PostId=\"511\" RevisionGUID=\"bd9459b2-c9f1-4aad-b3aa-98a034412551\" CreationDate=\"2014-06-22T10:03:50.983\" UserId=\"97\" Comment=\"Use more accurate term in header\" Text=\"&lt;cross-validation&gt;&lt;sampling&gt;\" />  <row Id=\"1445\" PostHistoryTypeId=\"2\" PostId=\"588\" RevisionGUID=\"160314c6-628a-4f16-a029-535a491eaa41\" CreationDate=\"2014-06-25T18:12:04.010\" UserId=\"97\" Text=\"Quant SE is better place for questions related to getting financial data:&#xD;&#xA;&#xD;&#xA; - [What data sources are available online][1]&#xD;&#xA; - http://quant.stackexchange.com/search?q=market+capitalization+data&#xD;&#xA;&#xD;&#xA;  [1]: http://quant.stackexchange.com/questions/141/what-data-sources-are-available-online\" />  <row Id=\"1446\" PostHistoryTypeId=\"4\" PostId=\"587\" RevisionGUID=\"3abf116f-8697-4822-8f08-3ffc4e725b6e\" CreationDate=\"2014-06-25T19:04:31.130\" UserId=\"97\" Comment=\"Adding more relevant tag. This question is probably off-topic.\" Text=\"Where can I download historical market capitalization and daily turnover data for stocks?\" />  <row Id=\"1447\" PostHistoryTypeId=\"6\" PostId=\"587\" RevisionGUID=\"3abf116f-8697-4822-8f08-3ffc4e725b6e\" CreationDate=\"2014-06-25T19:04:31.130\" UserId=\"97\" Comment=\"Adding more relevant tag. This question is probably off-topic.\" Text=\"&lt;data-sources&gt;\" />  <row Id=\"1529\" PostHistoryTypeId=\"6\" PostId=\"587\" RevisionGUID=\"35fe2905-e9e6-4bf5-841c-8d40a9fea929\" CreationDate=\"2014-06-26T16:12:02.640\" UserId=\"97\" Comment=\"Adding more relevant tags.\" Text=\"&lt;dataset&gt;&lt;data-sources&gt;\" />  <row Id=\"1596\" PostHistoryTypeId=\"2\" PostId=\"617\" RevisionGUID=\"11b9385f-3a52-4681-be73-c0a34713e585\" CreationDate=\"2014-06-27T16:40:17.393\" UserId=\"97\" Text=\"What do you think is distance measure in your case?&#xD;&#xA;&#xD;&#xA;I assume there are three dimensions here:&#xD;&#xA;&#xD;&#xA; - `RowN` (row number)&#xD;&#xA; - `ColN` (column number)&#xD;&#xA; - `Value` (value: A, B or C)&#xD;&#xA;&#xD;&#xA;Is `value` scaled? In other words, is `A &lt; B &lt; C`? &#xD;&#xA;&#xD;&#xA;If yes, then&#xD;&#xA;&#xD;&#xA; - you can replace `{A, B, C}` with `{0, 1, 2}` (or may be `{10, 11, 12}`, if you want this difference be less important than RowN and ColN attributes)&#xD;&#xA; - normalize your data&#xD;&#xA; - use, for example, K-Means clustering algorithm (http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html) from `stats` R package&#xD;&#xA;&#xD;&#xA;In that case the distance between two will be:&#xD;&#xA;&#xD;&#xA;    Sqrt( (RowN1-RowN2)^2 + (ColN1-ColN2)^2 + (Value1-Value2)^2 )&#xD;&#xA;&#xD;&#xA;If `value` is not scaled (regular categorical variable), use some [modifications of K-Means that work with categorical data][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data&#xD;&#xA;&#xD;&#xA;So in case of 100x100 matrix you have 10000 observations and three variables, which is pretty trivial sample size.\" />  <row Id=\"1597\" PostHistoryTypeId=\"5\" PostId=\"617\" RevisionGUID=\"74e8b163-0e19-4334-bddf-44bdedf8adf4\" CreationDate=\"2014-06-27T16:49:26.077\" UserId=\"97\" Comment=\"Add more details to answer.\" Text=\"What do you think is distance measure in your case?&#xD;&#xA;&#xD;&#xA;I assume there are three dimensions here:&#xD;&#xA;&#xD;&#xA; - `RowN` (row number)&#xD;&#xA; - `ColN` (column number)&#xD;&#xA; - `Value` (value: A, B or C)&#xD;&#xA;&#xD;&#xA;That means data you get from `4x5` matrix looks like:&#xD;&#xA;&#xD;&#xA;    Sample1 -&gt; (1, 1, A)&#xD;&#xA;    Sample2 -&gt; (1, 2, B)&#xD;&#xA;    ...&#xD;&#xA;    Sample5 -&gt; (1, 5, A)&#xD;&#xA;    Sample6 -&gt; (2, 1, A)&#xD;&#xA;    ...&#xD;&#xA;    Sample15 -&gt; (3, 5, C)&#xD;&#xA;    ...&#xD;&#xA;    Sample20 -&gt; (4, 5, A)&#xD;&#xA;&#xD;&#xA;Is `value` scaled? In other words, is `A &lt; B &lt; C`? &#xD;&#xA;&#xD;&#xA;If yes, then&#xD;&#xA;&#xD;&#xA; - you can replace `{A, B, C}` with `{0, 1, 2}` (or may be `{10, 11, 12}`, if you want this difference be less important than RowN and ColN attributes)&#xD;&#xA; - normalize your data&#xD;&#xA; - use, for example, K-Means clustering algorithm (http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html) from `stats` R package&#xD;&#xA;&#xD;&#xA;In that case the distance between two will be:&#xD;&#xA;&#xD;&#xA;    Sqrt( (RowN1-RowN2)^2 + (ColN1-ColN2)^2 + (Value1-Value2)^2 )&#xD;&#xA;&#xD;&#xA;If `value` is not scaled (regular categorical variable), use some [modifications of K-Means that work with categorical data][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data&#xD;&#xA;&#xD;&#xA;So in case of 100x100 matrix you have 10000 observations and three variables, which is pretty trivial sample size.\" />  <row Id=\"1601\" PostHistoryTypeId=\"4\" PostId=\"511\" RevisionGUID=\"7405d122-2937-442e-90e6-95b797fe6ae4\" CreationDate=\"2014-06-27T17:36:39.530\" UserId=\"97\" Comment=\"Get header back\" Text=\"Cross-validation: K-fold vs Repeated random sub-sampling\" />  <row Id=\"1865\" PostHistoryTypeId=\"6\" PostId=\"716\" RevisionGUID=\"e497a222-a887-4807-918d-071ebf6495c2\" CreationDate=\"2014-07-10T17:51:00.230\" UserId=\"97\" Comment=\"More relevant tags.\" Text=\"&lt;machine-learning&gt;&lt;neural-network&gt;&lt;feature-selection&gt;&lt;feature-extraction&gt;\" />  <row Id=\"1952\" PostHistoryTypeId=\"6\" PostId=\"739\" RevisionGUID=\"f9d259f3-e80c-4704-9133-56a222e8ca35\" CreationDate=\"2014-07-16T15:35:15.697\" UserId=\"97\" Comment=\"Re-tag to more relevant.\" Text=\"&lt;education&gt;&lt;definitions&gt;\" />  <row Id=\"2349\" PostHistoryTypeId=\"2\" PostId=\"919\" RevisionGUID=\"bea47f69-9a2d-4c97-81c9-8307f5ca692f\" CreationDate=\"2014-08-06T08:41:44.967\" UserId=\"97\" Text=\"Data set looks like:&#xD;&#xA;&#xD;&#xA;- 25000 observations&#xD;&#xA;- up to 15 predictors of different types: numeric, multi-class categorical, binary&#xD;&#xA;- target variable is binary&#xD;&#xA;&#xD;&#xA;Which cross validation method is typical for this type of problems?&#xD;&#xA;&#xD;&#xA;By default I\\'m using K-Fold. How many folds is enough in this case? (One of the models I use is random forest, which is time consuming...)\" />  <row Id=\"2350\" PostHistoryTypeId=\"1\" PostId=\"919\" RevisionGUID=\"bea47f69-9a2d-4c97-81c9-8307f5ca692f\" CreationDate=\"2014-08-06T08:41:44.967\" UserId=\"97\" Text=\"Which cross-validation type best suits to binary classification problem\" />  <row Id=\"2351\" PostHistoryTypeId=\"3\" PostId=\"919\" RevisionGUID=\"bea47f69-9a2d-4c97-81c9-8307f5ca692f\" CreationDate=\"2014-08-06T08:41:44.967\" UserId=\"97\" Text=\"&lt;classification&gt;&lt;cross-validation&gt;\" />  <row Id=\"2355\" PostHistoryTypeId=\"2\" PostId=\"921\" RevisionGUID=\"4b9e1a07-d551-41cb-b68f-af5456936955\" CreationDate=\"2014-08-06T09:03:20.857\" UserId=\"97\" Text=\"I am fitting a model in R.&#xD;&#xA;&#xD;&#xA;- use `createFolds` method to create several `k` folds from the data set&#xD;&#xA;- loop through the folds, repeating the following on each iteration:&#xD;&#xA;  - `train` the model on k-1 folds&#xD;&#xA;  - `predict` the outcomes for the i-th fold&#xD;&#xA;  - calculate prediction accuracy&#xD;&#xA;- average the accuracy&#xD;&#xA;&#xD;&#xA;Does R have a function that makes folds itself, repeats model tuning/predictions and gives the average accuracy back?\" />  <row Id=\"2356\" PostHistoryTypeId=\"1\" PostId=\"921\" RevisionGUID=\"4b9e1a07-d551-41cb-b68f-af5456936955\" CreationDate=\"2014-08-06T09:03:20.857\" UserId=\"97\" Text=\"Avoid iterations while calculating average model accuracy\" />  <row Id=\"2357\" PostHistoryTypeId=\"3\" PostId=\"921\" RevisionGUID=\"4b9e1a07-d551-41cb-b68f-af5456936955\" CreationDate=\"2014-08-06T09:03:20.857\" UserId=\"97\" Text=\"&lt;r&gt;&lt;accuracy&gt;&lt;cross-validation&gt;&lt;sampling&gt;&lt;beginner&gt;\" />  <row Id=\"2400\" PostHistoryTypeId=\"6\" PostId=\"893\" RevisionGUID=\"79d5bbc6-0da2-475c-aa93-8fedd8dc3612\" CreationDate=\"2014-08-07T23:38:37.457\" UserId=\"97\" Comment=\"Add relevant tag\" Text=\"&lt;r&gt;&lt;statistics&gt;&lt;correlation&gt;\" />  <row Id=\"2422\" PostHistoryTypeId=\"6\" PostId=\"936\" RevisionGUID=\"4a8b34d3-6b7b-4ade-9f43-2624f6e0effa\" CreationDate=\"2014-08-08T12:27:15.360\" UserId=\"97\" Comment=\"Adding relevant tags.\" Text=\"&lt;r&gt;&lt;error-handling&gt;\" />  <row Id=\"2596\" PostHistoryTypeId=\"6\" PostId=\"1002\" RevisionGUID=\"c4111f36-1fd8-4aeb-bfb8-5c2b98f62cb4\" CreationDate=\"2014-08-20T09:50:18.473\" UserId=\"97\" Comment=\"Added relevant tags.\" Text=\"&lt;machine-learning&gt;&lt;data-mining&gt;&lt;tools&gt;&lt;beginner&gt;\" />  <row Id=\"2598\" PostHistoryTypeId=\"6\" PostId=\"992\" RevisionGUID=\"8bf693ce-e8ac-4969-95dc-d001ce903041\" CreationDate=\"2014-08-20T11:06:54.447\" UserId=\"97\" Comment=\"More relevant tags.\" Text=\"&lt;data-mining&gt;&lt;classification&gt;&lt;binary&gt;\" />  <row Id=\"2600\" PostHistoryTypeId=\"6\" PostId=\"1003\" RevisionGUID=\"c8aaba30-05f0-420b-9bd8-dc228ee5428d\" CreationDate=\"2014-08-20T12:14:01.593\" UserId=\"97\" Comment=\"Added more relevant tags.\" Text=\"&lt;machine-learning&gt;&lt;dataset&gt;&lt;text-mining&gt;&lt;search&gt;\" />  <row Id=\"2627\" PostHistoryTypeId=\"4\" PostId=\"997\" RevisionGUID=\"54809862-f276-46ed-964e-fc708ea34cfe\" CreationDate=\"2014-08-21T11:52:31.890\" UserId=\"97\" Comment=\"More relevant tags.\" Text=\"Where can I find free spatio-temporal dataset for download?\" />  <row Id=\"2628\" PostHistoryTypeId=\"6\" PostId=\"997\" RevisionGUID=\"54809862-f276-46ed-964e-fc708ea34cfe\" CreationDate=\"2014-08-21T11:52:31.890\" UserId=\"97\" Comment=\"More relevant tags.\" Text=\"&lt;dataset&gt;&lt;open-source&gt;&lt;freebase&gt;\" />  <row Id=\"2672\" PostHistoryTypeId=\"6\" PostId=\"1025\" RevisionGUID=\"e1dea512-eb36-46a7-8ac5-c7f4a57c1e1c\" CreationDate=\"2014-08-24T03:31:06.623\" UserId=\"97\" Comment=\"Additional tags.\" Text=\"&lt;machine-learning&gt;&lt;algorithms&gt;\" />  <row Id=\"2731\" PostHistoryTypeId=\"6\" PostId=\"1053\" RevisionGUID=\"87a35c96-2f73-4be5-b717-aa9c8c913e37\" CreationDate=\"2014-08-28T16:28:30.453\" UserId=\"97\" Comment=\"Additional related tags.\" Text=\"&lt;tools&gt;&lt;visualization&gt;&lt;scala&gt;&lt;csv&gt;\" />  <row Id=\"2852\" PostHistoryTypeId=\"2\" PostId=\"1107\" RevisionGUID=\"35a9ce98-7cc6-4803-9fe2-3cad4780f8d9\" CreationDate=\"2014-09-12T15:20:51.767\" UserId=\"97\" Text=\"I have a classification problem with approximately 1000 positive and 10000 negative samples in training set. So this data set is quite unbalanced. Plain random forest is just trying to mark all test samples as a majority class.&#xD;&#xA;&#xD;&#xA;Some good answers about sub-sampling and weighted random forest are given here: http://datascience.stackexchange.com/questions/454/what-are-the-implications-for-training-a-tree-ensemble-with-highly-biased-datase&#xD;&#xA;&#xD;&#xA;Which classification methods besides RF can handle the problem in the best way?\" />  <row Id=\"2853\" PostHistoryTypeId=\"1\" PostId=\"1107\" RevisionGUID=\"35a9ce98-7cc6-4803-9fe2-3cad4780f8d9\" CreationDate=\"2014-09-12T15:20:51.767\" UserId=\"97\" Text=\"Quick guide into training highly imbalanced data sets\" />  <row Id=\"2854\" PostHistoryTypeId=\"3\" PostId=\"1107\" RevisionGUID=\"35a9ce98-7cc6-4803-9fe2-3cad4780f8d9\" CreationDate=\"2014-09-12T15:20:51.767\" UserId=\"97\" Text=\"&lt;machine-learning&gt;&lt;classification&gt;&lt;dataset&gt;&lt;unbalanced-classes&gt;\" />  <row Id=\"2855\" PostHistoryTypeId=\"2\" PostId=\"1108\" RevisionGUID=\"80256954-6698-4110-a4f5-e6ea4df86169\" CreationDate=\"2014-09-12T16:26:15.827\" UserId=\"97\" Text=\"As mentioned [before][1], I have a classification problem and unbalanced data set. Which contains 9-10 times more negative samples than positive.&#xD;&#xA;I have trained `&quot;gbm&quot;` Generalized Boosted Regression model from `CARET` package in `R` and get the following output:&#xD;&#xA;&#xD;&#xA;      interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD&#xD;&#xA;      1                  50       0.906     0.523  0.00978      0.0512  &#xD;&#xA;      1                  100      0.91      0.561  0.0108       0.0517  &#xD;&#xA;      1                  150      0.91      0.572  0.0104       0.0492  &#xD;&#xA;      2                  50       0.908     0.569  0.0106       0.0484  &#xD;&#xA;      2                  100      0.91      0.582  0.00965      0.0443  &#xD;&#xA;      2                  150      0.91      0.584  0.00976      0.0437  &#xD;&#xA;      3                  50       0.909     0.578  0.00996      0.0469  &#xD;&#xA;      3                  100      0.91      0.583  0.00975      0.0447  &#xD;&#xA;      3                  150      0.911     0.586  0.00962      0.0443  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Looking at the 90% accuracy I assume that model has labeled all the samples as majority class. That\\'s clear.&#xD;&#xA;And what is not transparent: how Kappa is calculated.&#xD;&#xA;&#xD;&#xA;- What does this Kappa values (near to 60%) really mean? Is it enough to say that the model is not classifying them just by chance? &#xD;&#xA;- What do `Accuracy SD` and `Kappa SD` stand for?&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets\" />  <row Id=\"2856\" PostHistoryTypeId=\"1\" PostId=\"1108\" RevisionGUID=\"80256954-6698-4110-a4f5-e6ea4df86169\" CreationDate=\"2014-09-12T16:26:15.827\" UserId=\"97\" Text=\"Kappa near to 60% in unbalanced (1:10) data set\" />  <row Id=\"2857\" PostHistoryTypeId=\"3\" PostId=\"1108\" RevisionGUID=\"80256954-6698-4110-a4f5-e6ea4df86169\" CreationDate=\"2014-09-12T16:26:15.827\" UserId=\"97\" Text=\"&lt;r&gt;&lt;unbalanced-classes&gt;&lt;gbm&gt;\" />  <row Id=\"2879\" PostHistoryTypeId=\"5\" PostId=\"1108\" RevisionGUID=\"6324f8a0-6ec1-4803-b70e-47621f148486\" CreationDate=\"2014-09-15T07:31:57.053\" UserId=\"97\" Comment=\"deleted 15 characters in body\" Text=\"As mentioned [before][1], I have a classification problem and unbalanced data set. The majority class contains 88% of all samples.&#xD;&#xA;I have trained `&quot;gbm&quot;` Generalized Boosted Regression model from `CARET` package in `R` and get the following output:&#xD;&#xA;&#xD;&#xA;      interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD&#xD;&#xA;      1                  50       0.906     0.523  0.00978      0.0512  &#xD;&#xA;      1                  100      0.91      0.561  0.0108       0.0517  &#xD;&#xA;      1                  150      0.91      0.572  0.0104       0.0492  &#xD;&#xA;      2                  50       0.908     0.569  0.0106       0.0484  &#xD;&#xA;      2                  100      0.91      0.582  0.00965      0.0443  &#xD;&#xA;      2                  150      0.91      0.584  0.00976      0.0437  &#xD;&#xA;      3                  50       0.909     0.578  0.00996      0.0469  &#xD;&#xA;      3                  100      0.91      0.583  0.00975      0.0447  &#xD;&#xA;      3                  150      0.911     0.586  0.00962      0.0443  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Looking at the 90% accuracy I assume that model has labeled all the samples as majority class. That\\'s clear.&#xD;&#xA;And what is not transparent: how Kappa is calculated.&#xD;&#xA;&#xD;&#xA;- What does this Kappa values (near to 60%) really mean? Is it enough to say that the model is not classifying them just by chance? &#xD;&#xA;- What do `Accuracy SD` and `Kappa SD` mean?&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets\" />  <row Id=\"2883\" PostHistoryTypeId=\"33\" PostId=\"1108\" RevisionGUID=\"a14ea099-5846-4145-994a-8baf462bd614\" CreationDate=\"2014-09-15T11:28:09.737\" UserId=\"97\" Comment=\"6\" />  <row Id=\"2885\" PostHistoryTypeId=\"2\" PostId=\"1118\" RevisionGUID=\"0e7e00dd-8416-4968-9570-7ca0bb8f8d0d\" CreationDate=\"2014-09-15T13:03:44.083\" UserId=\"97\" Text=\"There are several approaches. You can start from the second one.&#xD;&#xA;&#xD;&#xA;**\\x84 Equal-width (distance) partitioning:**&#xD;&#xA;&#xD;&#xA;- It divides the range into N intervals of equal size: uniform grid&#xD;&#xA;&#xD;&#xA;- if A and B are the lowest and highest values of the attribute, the width of intervals will be: `W = (B-A)/N`.&#xD;&#xA;&#xD;&#xA;- The most straightforward&#xD;&#xA;\\x89- Outliers may dominate presentation&#xD;&#xA;\\x89- Skewed data is not handled well.&#xD;&#xA;&#xD;&#xA;**\\x84 Equal-depth (frequency) partitioning:**&#xD;&#xA;&#xD;&#xA;- It divides the range into &#xD;&#xA;N intervals, each containing approximately &#xD;&#xA;same number of samples&#xD;&#xA;- Good data scaling&#xD;&#xA;- Managing categorical attributes can be tricky.&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;**Other Methods**&#xD;&#xA;&#xD;&#xA;- `Rank`: The rank of a number is its size relative to other values of a numerical variable. First, we sort the list of values, then we assign the position of a value as its rank. Same values receive the same rank but the presence of duplicate values affects the ranks of subsequent values (e.g., 1,2,3,3,5). Rank is a solid binning method with one major drawback, values can have different ranks in different lists.&#xD;&#xA;- `Quantiles (median, quartiles, percentiles, ...)`: Quantiles are also very useful binning methods but like Rank, one value can have different quantile if the list of values changes.&#xD;&#xA;- `Math functions`: For example, logarithmic binning is an effective method for the numerical variables with highly skewed distribution (e.g., income).&#xD;&#xA;&#xD;&#xA;**Entropy-based Binning**&#xD;&#xA;&#xD;&#xA;[Entropy based method][1] uses a split approach. The entropy (or the information content) is calculated based on the class label. Intuitively, it finds the best split so that the bins are as pure as possible that is the majority of the values in a bin correspond to have the same class label. Formally, it is characterized by finding the split with the maximal information gain. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.saedsayad.com/supervised_binning.htm\" />  <row Id=\"2886\" PostHistoryTypeId=\"2\" PostId=\"1119\" RevisionGUID=\"cb72c3ae-20b6-445a-bad0-3b1a947688ac\" CreationDate=\"2014-09-15T13:14:40.797\" UserId=\"97\" Text=\"[RFM][1] - is a ranking model when all customers are ranked according to their purchasing **F** requency, **R** recency and **M** monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.&#xD;&#xA;&#xD;&#xA;The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/RFM_(customer_value)\" />  <row Id=\"2887\" PostHistoryTypeId=\"1\" PostId=\"1119\" RevisionGUID=\"cb72c3ae-20b6-445a-bad0-3b1a947688ac\" CreationDate=\"2014-09-15T13:14:40.797\" UserId=\"97\" Text=\"Predictive modeling based on RFM scoring indicators\" />  <row Id=\"2888\" PostHistoryTypeId=\"3\" PostId=\"1119\" RevisionGUID=\"cb72c3ae-20b6-445a-bad0-3b1a947688ac\" CreationDate=\"2014-09-15T13:14:40.797\" UserId=\"97\" Text=\"&lt;rmf-scoring&gt;&lt;marketing&gt;&lt;predictive-modeling&gt;\" />  <row Id=\"2890\" PostHistoryTypeId=\"5\" PostId=\"1119\" RevisionGUID=\"33c9d010-fe35-4e0b-90f0-c048922cec12\" CreationDate=\"2014-09-15T20:03:21.770\" UserId=\"97\" Comment=\"Make edits based on feedback in comments\" Text=\"[RFM][1] - is a ranking model when all customers are ranked according to their purchasing **F** requency, **R** recency and **M** monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.&#xD;&#xA;&#xD;&#xA;The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?&#xD;&#xA;&#xD;&#xA;Edit:&#xD;&#xA;- predicting which customer will most likely spend more&#xD;&#xA;- who is going to upgrade/renew subscribtion/refund etc&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/RFM_(customer_value)\" />  <row Id=\"2910\" PostHistoryTypeId=\"6\" PostId=\"1110\" RevisionGUID=\"ad1ead95-e404-4a16-9b3a-df1c23120b29\" CreationDate=\"2014-09-16T15:54:44.060\" UserId=\"97\" Comment=\"Add more relevant tags\" Text=\"&lt;clustering&gt;&lt;k-means&gt;\" />  <row Id=\"2928\" PostHistoryTypeId=\"2\" PostId=\"1133\" RevisionGUID=\"b7bf7a35-f73c-401e-a8e2-e331ffd24923\" CreationDate=\"2014-09-17T10:16:07.100\" UserId=\"97\" Text=\"The solution is described here: http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data&#xD;&#xA;&#xD;&#xA;C# implementation can be found in ALGLIB library, which I strongly recommend: http://www.alglib.net/translator/man/manual.csharp.html#gs_packages\" />  <row Id=\"2947\" PostHistoryTypeId=\"2\" PostId=\"1142\" RevisionGUID=\"46145d2d-6804-45d5-b521-254aad8a3c44\" CreationDate=\"2014-09-19T07:15:13.803\" UserId=\"97\" Text=\"First of all I should say you question probably is an off-topic and will be closed soon.&#xD;&#xA;&#xD;&#xA;Anyway I can target you to similar questions discussed at this SE site already:&#xD;&#xA;&#xD;&#xA;- http://datascience.stackexchange.com/questions/808/statistics-computer-science-data-science&#xD;&#xA;&#xD;&#xA;- http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require&#xD;&#xA;&#xD;&#xA;A set of relevant questions at Cross Validated Stack Exchange:&#xD;&#xA;&#xD;&#xA;- http://stats.stackexchange.com/questions/tagged/careers&#xD;&#xA;&#xD;&#xA;This is good infographics of data science knowledge you might need to start a career:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/tJJ2x.png\" />  <row Id=\"2948\" PostHistoryTypeId=\"5\" PostId=\"1142\" RevisionGUID=\"17ede6c8-7fea-4f83-bc52-97998c358c4d\" CreationDate=\"2014-09-19T07:23:47.883\" UserId=\"97\" Comment=\"More inforamation\" Text=\"First of all I should say you question probably is an off-topic and will be closed soon.&#xD;&#xA;&#xD;&#xA;**Discussed at this SE site**&#xD;&#xA;&#xD;&#xA;Anyway I can target you to similar questions discussed at this SE site already:&#xD;&#xA;&#xD;&#xA;- http://datascience.stackexchange.com/questions/808/statistics-computer-science-data-science&#xD;&#xA;&#xD;&#xA;- http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require&#xD;&#xA;&#xD;&#xA;**Cross Validated SE**&#xD;&#xA;&#xD;&#xA;A set of relevant questions at Cross Validated Stack Exchange:&#xD;&#xA;&#xD;&#xA;- http://stats.stackexchange.com/questions/tagged/careers&#xD;&#xA;&#xD;&#xA;**Data scientist map**&#xD;&#xA;&#xD;&#xA;This is good infographics of data science knowledge you might need to start a career:&#xD;&#xA;&#xD;&#xA;![This is good infographics of data science knowledge you might need to start a career][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Careers SE**&#xD;&#xA;&#xD;&#xA;Also, simple &quot;data scientist&quot; querying of Careers SE site &#xD;&#xA;http://careers.stackoverflow.com/jobs?searchTerm=data+scientist&amp;location= will lead you to the following knowlege &quot;tags&quot;:&#xD;&#xA;&#xD;&#xA;- [R][2]&#xD;&#xA;- [bigdata][3]&#xD;&#xA;- [data-visualization][4]&#xD;&#xA;- [hadoop][5]&#xD;&#xA;- [mapreduce][6]&#xD;&#xA;- [scala][7]&#xD;&#xA;- [python][8]&#xD;&#xA;- [matlab][9]&#xD;&#xA;&#xD;&#xA;etc.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/tJJ2x.png&#xD;&#xA;  [2]: http://careers.stackoverflow.com/jobs/tag/r&#xD;&#xA;  [3]: http://careers.stackoverflow.com/jobs/tag/bigdata&#xD;&#xA;  [4]: http://careers.stackoverflow.com/jobs/tag/data-visualization&#xD;&#xA;  [5]: http://careers.stackoverflow.com/jobs/tag/hadoop&#xD;&#xA;  [6]: http://careers.stackoverflow.com/jobs/tag/mapreduce&#xD;&#xA;  [7]: http://careers.stackoverflow.com/jobs/tag/scala&#xD;&#xA;  [8]: http://careers.stackoverflow.com/jobs/tag/python&#xD;&#xA;  [9]: http://careers.stackoverflow.com/jobs/tag/matlab\" />  <row Id=\"2949\" PostHistoryTypeId=\"5\" PostId=\"1142\" RevisionGUID=\"9ec20645-4e1b-4d48-ac15-c75a77498b28\" CreationDate=\"2014-09-19T09:04:24.007\" UserId=\"97\" Comment=\"add link to higher resolution picture\" Text=\"First of all I should say you question probably is an off-topic and will be closed soon.&#xD;&#xA;&#xD;&#xA;**Discussed at this SE site**&#xD;&#xA;&#xD;&#xA;Anyway I can target you to similar questions discussed at this SE site already:&#xD;&#xA;&#xD;&#xA;- http://datascience.stackexchange.com/questions/808/statistics-computer-science-data-science&#xD;&#xA;&#xD;&#xA;- http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require&#xD;&#xA;&#xD;&#xA;**Cross Validated SE**&#xD;&#xA;&#xD;&#xA;A set of relevant questions at Cross Validated Stack Exchange:&#xD;&#xA;&#xD;&#xA;- http://stats.stackexchange.com/questions/tagged/careers&#xD;&#xA;&#xD;&#xA;**Data scientist map**&#xD;&#xA;&#xD;&#xA;This is good infographics of data science knowledge you might need to start a career ([Link to image][1]):&#xD;&#xA;&#xD;&#xA;![This is good infographics of data science knowledge you might need to start a career][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Careers SE**&#xD;&#xA;&#xD;&#xA;Also, simple &quot;data scientist&quot; querying of Careers SE site &#xD;&#xA;http://careers.stackoverflow.com/jobs?searchTerm=data+scientist&amp;location= will lead you to the following knowlege &quot;tags&quot;:&#xD;&#xA;&#xD;&#xA;- [R][3]&#xD;&#xA;- [bigdata][4]&#xD;&#xA;- [data-visualization][5]&#xD;&#xA;- [hadoop][6]&#xD;&#xA;- [mapreduce][7]&#xD;&#xA;- [scala][8]&#xD;&#xA;- [python][9]&#xD;&#xA;- [matlab][10]&#xD;&#xA;&#xD;&#xA;etc.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://pennlio.files.wordpress.com/2013/09/roadtodatascientist1.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/tJJ2x.png&#xD;&#xA;  [3]: http://careers.stackoverflow.com/jobs/tag/r&#xD;&#xA;  [4]: http://careers.stackoverflow.com/jobs/tag/bigdata&#xD;&#xA;  [5]: http://careers.stackoverflow.com/jobs/tag/data-visualization&#xD;&#xA;  [6]: http://careers.stackoverflow.com/jobs/tag/hadoop&#xD;&#xA;  [7]: http://careers.stackoverflow.com/jobs/tag/mapreduce&#xD;&#xA;  [8]: http://careers.stackoverflow.com/jobs/tag/scala&#xD;&#xA;  [9]: http://careers.stackoverflow.com/jobs/tag/python&#xD;&#xA;  [10]: http://careers.stackoverflow.com/jobs/tag/matlab\" />  <row Id=\"2950\" PostHistoryTypeId=\"5\" PostId=\"1132\" RevisionGUID=\"97e05d34-1be7-4b65-8dfd-705dae268134\" CreationDate=\"2014-09-19T12:15:42.527\" UserId=\"97\" Comment=\"Retag, reformulate.\" Text=\"I am a research scholar in data mining. I\\'m interested in C# implementation of K-Means clustering algorithm for mixed numeric and categorical data.\" />  <row Id=\"2951\" PostHistoryTypeId=\"4\" PostId=\"1132\" RevisionGUID=\"97e05d34-1be7-4b65-8dfd-705dae268134\" CreationDate=\"2014-09-19T12:15:42.527\" UserId=\"97\" Comment=\"Retag, reformulate.\" Text=\"K-Means clustering for mixed numeric and categorical data implementation in C#\" />  <row Id=\"2952\" PostHistoryTypeId=\"6\" PostId=\"1132\" RevisionGUID=\"97e05d34-1be7-4b65-8dfd-705dae268134\" CreationDate=\"2014-09-19T12:15:42.527\" UserId=\"97\" Comment=\"Retag, reformulate.\" Text=\"&lt;clustering&gt;&lt;k-means&gt;&lt;beginner&gt;&lt;library&gt;\" />  <row Id=\"2956\" PostHistoryTypeId=\"4\" PostId=\"1141\" RevisionGUID=\"f7f47f2e-b958-4a2c-88f9-066f9f9d9b7d\" CreationDate=\"2014-09-19T12:19:25.697\" UserId=\"97\" Comment=\"Add relevant tags.\" Text=\"Some suggestion for career in data science or predictive modeling\" />  <row Id=\"2957\" PostHistoryTypeId=\"6\" PostId=\"1141\" RevisionGUID=\"f7f47f2e-b958-4a2c-88f9-066f9f9d9b7d\" CreationDate=\"2014-09-19T12:19:25.697\" UserId=\"97\" Comment=\"Add relevant tags.\" Text=\"&lt;education&gt;&lt;beginner&gt;&lt;career&gt;\" />  <row Id=\"2976\" PostHistoryTypeId=\"34\" PostId=\"1108\" RevisionGUID=\"d33e6f10-9261-40d2-b006-56237df70c72\" CreationDate=\"2014-09-21T18:25:47.173\" UserId=\"97\" Comment=\"6\" />  <row Id=\"2985\" PostHistoryTypeId=\"5\" PostId=\"1119\" RevisionGUID=\"ab747210-9454-46be-9f90-0fc9089d0d92\" CreationDate=\"2014-09-23T08:57:41.193\" UserId=\"97\" Comment=\"Add more information, corrected spelling error in tag.\" Text=\"[RFM][1] - is a ranking model when all customers are ranked according to their purchasing **F** requency, **R** recency and **M** monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.&#xD;&#xA;&#xD;&#xA;The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?&#xD;&#xA;&#xD;&#xA;**Update**:&#xD;&#xA;&#xD;&#xA;- predicting which customer will most likely spend more&#xD;&#xA;- who is going to upgrade/renew subscribtion/refund etc&#xD;&#xA;&#xD;&#xA;**Update2**:&#xD;&#xA;&#xD;&#xA;- I understand, this simple problem with three independent variable and one classifier. My guess and experience say these pure three factors do not predict future customer value. But they can be used together with another data or can be an additional input into some model.&#xD;&#xA;- Please share which methodologies worked for you personally and are likely to have high predictive ability. What kind of data you used together with RFM indicators and it worked well?&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/RFM_(customer_value)\" />  <row Id=\"2986\" PostHistoryTypeId=\"33\" PostId=\"1119\" RevisionGUID=\"3743307a-43c4-4952-984f-6d2a393e4de4\" CreationDate=\"2014-09-23T08:58:09.600\" UserId=\"97\" Comment=\"8\" />  <row Id=\"2987\" PostHistoryTypeId=\"6\" PostId=\"1119\" RevisionGUID=\"ab747210-9454-46be-9f90-0fc9089d0d92\" CreationDate=\"2014-09-23T08:57:41.193\" UserId=\"97\" Comment=\"Add more information, corrected spelling error in tag.\" Text=\"&lt;marketing&gt;&lt;predictive-modeling&gt;\" />  <row Id=\"2988\" PostHistoryTypeId=\"5\" PostId=\"1119\" RevisionGUID=\"6b4fe253-ae05-49e7-9ba7-a17b4053c8db\" CreationDate=\"2014-09-23T09:11:21.413\" UserId=\"97\" Comment=\"minor changes in spelling\" Text=\"[RFM][1] - is a ranking model when all customers are ranked according to their purchasing **F** requency, **R** recency and **M** monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.&#xD;&#xA;&#xD;&#xA;The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?&#xD;&#xA;&#xD;&#xA;**Update**:&#xD;&#xA;&#xD;&#xA;- predicting which customer will most likely spend more&#xD;&#xA;- who is going to upgrade/renew subscribtion/refund etc&#xD;&#xA;&#xD;&#xA;**Update2**:&#xD;&#xA;&#xD;&#xA;- I understand, this is simple problem with three independent variable and one classifier. My guess and experience say these pure three factors do not predict future customer value. But they can be used together with another data or can be an additional input into some model.&#xD;&#xA;- Please share which methodologies worked for you personally and are likely to have high predictive ability. What kind of data you used together with RFM indicators and it worked well?&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/RFM_(customer_value)\" />  <row Id=\"4675\" PostHistoryTypeId=\"6\" PostId=\"1059\" RevisionGUID=\"2cf993fe-f532-466b-82a2-63ffb331a4fc\" CreationDate=\"2014-11-07T08:28:10.637\" UserId=\"97\" Comment=\"Add more relevant tags\" Text=\"&lt;r&gt;&lt;neural-network&gt;&lt;time-series&gt;&lt;forecast&gt;\" />  <row Id=\"4862\" PostHistoryTypeId=\"2\" PostId=\"2491\" RevisionGUID=\"7e6cb5e7-8ac2-444f-9c77-90c4905d472f\" CreationDate=\"2014-11-18T09:56:06.157\" UserId=\"97\" Text=\"Data sample contains a single feature: random integer number from 1 to 4.&#xD;&#xA;&#xD;&#xA;Is it possble to change `1,2,3,4` representation on the filter card to some custom names, say: `Type1,Type2,Type3,Type4`&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/4ZPYM.png\" />  <row Id=\"4863\" PostHistoryTypeId=\"1\" PostId=\"2491\" RevisionGUID=\"7e6cb5e7-8ac2-444f-9c77-90c4905d472f\" CreationDate=\"2014-11-18T09:56:06.157\" UserId=\"97\" Text=\"Change aliases of filter items in Tableau\" />  <row Id=\"4864\" PostHistoryTypeId=\"3\" PostId=\"2491\" RevisionGUID=\"7e6cb5e7-8ac2-444f-9c77-90c4905d472f\" CreationDate=\"2014-11-18T09:56:06.157\" UserId=\"97\" Text=\"&lt;visualization&gt;&lt;tableau&gt;\" />  <row Id=\"4865\" PostHistoryTypeId=\"5\" PostId=\"2491\" RevisionGUID=\"dc4a64d3-2342-4be3-8390-82c3c48a71a7\" CreationDate=\"2014-11-18T10:02:47.110\" UserId=\"97\" Comment=\"some details \" Text=\"Data sample contains a single feature: random integer number from 1 to 4.&#xD;&#xA;&#xD;&#xA;Is it possble to change `1,2,3,4` representation on the filter card to some custom names, say: `Type1,Type2,Type3,Type4`? (not changing data set)&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/4ZPYM.png\" />  <row Id=\"5027\" PostHistoryTypeId=\"4\" PostId=\"2543\" RevisionGUID=\"348510c4-9a12-4eac-95f0-fb6d9ef53af2\" CreationDate=\"2014-11-26T09:35:10.677\" UserId=\"97\" Comment=\"Change to more relevant title\" Text=\"Can Machine Learning be applied in software developement\" />  <row Id=\"5029\" PostHistoryTypeId=\"2\" PostId=\"2544\" RevisionGUID=\"a0a205cb-eaf3-420e-8e2f-4aab559c1bec\" CreationDate=\"2014-11-26T09:37:43.970\" UserId=\"97\" Text=\"Definately - Yes.&#xD;&#xA;Good question. &#xD;&#xA;I was thinking about it myself.&#xD;&#xA;&#xD;&#xA;**(1) Collect the data**.&#xD;&#xA;The first problem you have: gather enough data. &#xD;&#xA;All the attributes you mentioned (date, name, check-in title/comment, N of deffects etc) are potentially useful - gather as much as possible.&#xD;&#xA;As soon as you have a big project, a number of developers, many branches, frequent commits and you have started collecting all the data, you a ready to go further.&#xD;&#xA;&#xD;&#xA;**(2) Ask good questions**.&#xD;&#xA;The next question you should ask yourself: what effect are you going to measure, estimate and maybe predict.&#xD;&#xA;Frequency of possible bugs? Tracking inaccurate &quot;committers&quot;? Risky branches?&#xD;&#xA;&#xD;&#xA;**(3) Select the model**.&#xD;&#xA;As soon as you get the questions formulated, you should follow the general approach at data science - extract needed features in your data, select appropriate model, train you model and apply it.\" />  <row Id=\"5030\" PostHistoryTypeId=\"6\" PostId=\"2543\" RevisionGUID=\"c82923ef-a50c-4c01-939e-644e9ed8a150\" CreationDate=\"2014-11-26T09:43:21.757\" UserId=\"97\" Comment=\"Retagging to relevant tags\" Text=\"&lt;machine-learning&gt;&lt;recommendation&gt;&lt;predictive-modeling&gt;&lt;software-develpment&gt;\" />  <row Id=\"5032\" PostHistoryTypeId=\"5\" PostId=\"2544\" RevisionGUID=\"b8fc1855-7215-4d27-b9d6-1163c6b11357\" CreationDate=\"2014-11-26T09:52:45.610\" UserId=\"97\" Comment=\"More detatils.\" Text=\"Definately - Yes.&#xD;&#xA;Good question. &#xD;&#xA;Was thinking about it myself.&#xD;&#xA;&#xD;&#xA;**(1) Collect the data**.&#xD;&#xA;The first problem you have: gather enough data. &#xD;&#xA;All the attributes you mentioned (date, name, check-in title/comment, N of deffects etc) are potentially useful - gather as much as possible.&#xD;&#xA;As soon as you have a big project, a number of developers, many branches, frequent commits and you have started collecting all the data, you a ready to go further.&#xD;&#xA;&#xD;&#xA;**(2) Ask good questions**.&#xD;&#xA;The next question you should ask yourself: what effect are you going to measure, estimate and maybe predict.&#xD;&#xA;Frequency of possible bugs? Tracking inaccurate &quot;committers&quot;? Risky branches? Want to see some groups of users/bugs/commits according to some metrics?&#xD;&#xA;&#xD;&#xA;**(3) Select the model**.&#xD;&#xA;As soon as you get the questions formulated, you should follow the general approach at data science - extract needed features in your data, select appropriate model, train you model and test it, apply it.\" />  <row Id=\"5034\" PostHistoryTypeId=\"5\" PostId=\"2544\" RevisionGUID=\"f7b87384-9be8-4f33-9c03-03c41318fb36\" CreationDate=\"2014-11-26T09:58:47.527\" UserId=\"97\" Comment=\"More detatils.\" Text=\"Definately - Yes.&#xD;&#xA;Good question. &#xD;&#xA;Was thinking about it myself.&#xD;&#xA;&#xD;&#xA;**(1) Collect the data**.&#xD;&#xA;The first problem you have: gather enough data. &#xD;&#xA;All the attributes you mentioned (date, name, check-in title/comment, N of deffects etc) are potentially useful - gather as much as possible.&#xD;&#xA;As soon as you have a big project, a number of developers, many branches, frequent commits and you have started collecting all the data, you a ready to go further.&#xD;&#xA;&#xD;&#xA;**(2) Ask good questions**.&#xD;&#xA;The next question you should ask yourself: what effect are you going to measure, estimate and maybe predict.&#xD;&#xA;Frequency of possible bugs? Tracking inaccurate &quot;committers&quot;? Risky branches? Want to see some groups of users/bugs/commits according to some metrics?&#xD;&#xA;&#xD;&#xA;**(3) Select the model**.&#xD;&#xA;As soon as you have the questions formulated, you should follow the general approach in data science - extract needed features in your data, select appropriate model, train you model and test it, apply it. This is too broad process to discuss it this thread, so please use this site to get right answers.\" />  <row Id=\"5069\" PostHistoryTypeId=\"2\" PostId=\"2563\" RevisionGUID=\"ce7d9cf0-327d-4fd0-a8a9-84634880889d\" CreationDate=\"2014-11-28T09:19:32.867\" UserId=\"97\" Text=\"I have a visualization problem.&#xD;&#xA;&#xD;&#xA;Creating a comparison report of PR event efficiency. Say, show or exhibition.&#xD;&#xA;&#xD;&#xA;There are two dimensions of comparison:&#xD;&#xA;&#xD;&#xA;- compare vs the same event performance in the past years &#xD;&#xA;- compare vs another type of analogical/competitive events &#xD;&#xA;&#xD;&#xA;There is also a number of comparison aspects:&#xD;&#xA;&#xD;&#xA;- Audience&#xD;&#xA;- Media Coverage&#xD;&#xA;- Social Buzz&#xD;&#xA;- ROI&#xD;&#xA;- .... etc&#xD;&#xA;&#xD;&#xA;Each aspect is a set of some final KPI-s (just numbers, which can be compared vs another &quot;dimensions&quot;), plus maybe some descriptive text and pictures (which couldn\\'t be a metric but should be attached to the report).&#xD;&#xA;&#xD;&#xA;So finaly it looks like a three-dimensional coube:&#xD;&#xA;&#xD;&#xA; 1. Years&#xD;&#xA; 2. Another Events&#xD;&#xA; 3. Aspects&#xD;&#xA;&#xD;&#xA;If I put it in plain Word or PPT it will look like a document with dozen of slides/papers and linear structure.&#xD;&#xA;&#xD;&#xA;Any ideas how to compile an elegant user-friendly report?\" />  <row Id=\"5070\" PostHistoryTypeId=\"1\" PostId=\"2563\" RevisionGUID=\"ce7d9cf0-327d-4fd0-a8a9-84634880889d\" CreationDate=\"2014-11-28T09:19:32.867\" UserId=\"97\" Text=\"Visualization of three-dimensional report\" />  <row Id=\"5071\" PostHistoryTypeId=\"3\" PostId=\"2563\" RevisionGUID=\"ce7d9cf0-327d-4fd0-a8a9-84634880889d\" CreationDate=\"2014-11-28T09:19:32.867\" UserId=\"97\" Text=\"&lt;marketing&gt;&lt;infographics&gt;&lt;visualization&gt;\" />  <row Id=\"5088\" PostHistoryTypeId=\"6\" PostId=\"2563\" RevisionGUID=\"4d17ec07-4c52-449a-9306-ed130098b3de\" CreationDate=\"2014-11-30T08:44:27.780\" UserId=\"97\" Comment=\"retagging//\" Text=\"&lt;marketing&gt;&lt;infographics&gt;&lt;visualization&gt;\" />  <row Id=\"5289\" PostHistoryTypeId=\"2\" PostId=\"2656\" RevisionGUID=\"5d5934d7-b9a7-494f-9c2d-2b344e1acf87\" CreationDate=\"2014-12-09T08:30:28.000\" UserId=\"97\" Text=\"Found it myself.&#xD;&#xA;&#xD;&#xA;- Go to context menu right clicking to the dimension field.&#xD;&#xA;- Go to **Aliases...** and change the labels.&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/qKq73.png\" />  <row Id=\"6552\" PostHistoryTypeId=\"6\" PostId=\"3742\" RevisionGUID=\"dad3c02a-eb1f-4456-9ed5-934d4b102e94\" CreationDate=\"2014-12-22T15:41:37.473\" UserId=\"97\" Comment=\"Additional tag.\" Text=\"&lt;data-mining&gt;&lt;r&gt;&lt;dataset&gt;&lt;beginner&gt;\" />  <row Id=\"7850\" PostHistoryTypeId=\"6\" PostId=\"1216\" RevisionGUID=\"af247a05-4100-4ca8-a12c-86fff65698ca\" CreationDate=\"2015-01-10T11:10:53.493\" UserId=\"97\" Comment=\"Retagging to more relevant \" Text=\"&lt;career&gt;\" />  <row Id=\"8028\" PostHistoryTypeId=\"2\" PostId=\"4917\" RevisionGUID=\"5aed2bd3-43eb-4640-92f1-f9459356a7d7\" CreationDate=\"2015-01-21T09:15:54.750\" UserId=\"97\" Text=\"I want to visualize goal achievment progress.&#xD;&#xA;This is my first idea:&#xD;&#xA;&#xD;&#xA;- use area chart to show progress in current metric&#xD;&#xA;- use horizontal band to show the goal value&#xD;&#xA;- colorize areas under/above the band into &quot;positive&quot; and &quot;negative&quot; colors&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/h7fUa.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/EIhmz.png&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Is this approach informative enough? Are there better choises?**&#xD;&#xA;&#xD;&#xA;Additional info:&#xD;&#xA;&#xD;&#xA;- charts made in Tableau&#xD;&#xA;- two data sources: metric progress &amp; goals\" />  <row Id=\"8029\" PostHistoryTypeId=\"1\" PostId=\"4917\" RevisionGUID=\"5aed2bd3-43eb-4640-92f1-f9459356a7d7\" CreationDate=\"2015-01-21T09:15:54.750\" UserId=\"97\" Text=\"Visualize performance, % of goal implementation\" />  <row Id=\"8030\" PostHistoryTypeId=\"3\" PostId=\"4917\" RevisionGUID=\"5aed2bd3-43eb-4640-92f1-f9459356a7d7\" CreationDate=\"2015-01-21T09:15:54.750\" UserId=\"97\" Text=\"&lt;visualization&gt;&lt;marketing&gt;&lt;tableau&gt;\" />  <row Id=\"8104\" PostHistoryTypeId=\"2\" PostId=\"4948\" RevisionGUID=\"ce3a051a-a488-4161-ab69-d7c891aa1dc9\" CreationDate=\"2015-01-26T14:45:43.053\" UserId=\"97\" Text=\"Some good answers have already been posted at this site:&#xD;&#xA;&#xD;&#xA;- http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets&#xD;&#xA;&#xD;&#xA;And on Stats SE:&#xD;&#xA;&#xD;&#xA;- http://stats.stackexchange.com/questions/81111/classification-problem-using-imbalanced-dataset&#xD;&#xA;- http://stats.stackexchange.com/questions/16050/how-to-handle-data-imbalance-in-classification&#xD;&#xA;- http://stats.stackexchange.com/questions/60180/testing-classification-on-oversampled-imbalance-data?rq=1\" />  <row Id=\"8109\" PostHistoryTypeId=\"6\" PostId=\"4944\" RevisionGUID=\"f19d6e19-677f-4959-b6d0-f5e389713bd6\" CreationDate=\"2015-01-26T16:43:42.273\" UserId=\"97\" Comment=\"Add more relevant tags\" Text=\"&lt;machine-learning&gt;&lt;classification&gt;&lt;unbalanced-classes&gt;\" />  <row Id=\"8242\" PostHistoryTypeId=\"2\" PostId=\"5008\" RevisionGUID=\"1ec9dcfa-184e-481e-a69a-f2d155a9597f\" CreationDate=\"2015-01-31T14:27:15.657\" UserId=\"97\" Text=\"A huge list of open data sets is listed here:&#xD;&#xA;&#xD;&#xA;- http://datascience.stackexchange.com/questions/155/publicly-available-datasets&#xD;&#xA;&#xD;&#xA;Including Amazon, KDnuggets, Stanford, Twitter, Freebase, Google Public and more.\" />  <row Id=\"8269\" PostHistoryTypeId=\"2\" PostId=\"5022\" RevisionGUID=\"ec329fac-c4a2-45d2-b706-5d7eeedf40e7\" CreationDate=\"2015-02-03T08:09:44.953\" UserId=\"97\" Text=\"**Prediction**&#xD;&#xA;&#xD;&#xA;If the main goal is predicting anything, say, the statisitcs of the player in the next game, game result, then I would not recommend to do any scoring. Better way to go is using the pure statistics data as an input to the model. Any scoring/rankning - is information loss.&#xD;&#xA;&#xD;&#xA;**Ranking** &#xD;&#xA;&#xD;&#xA;If the goal is ranking itself, than you still need to have some target variable to predict. As you may want to check real predictive value of those ranks. That could be, again, playser stats in the *next* game or game result itself.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;*References*&#xD;&#xA;&#xD;&#xA;[Sport scores prediction][1] and [RFM scoring][2] are probably the next directions for you to look at.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/265/can-machine-learning-algorithms-predict-sports-scores-or-plays/269#269&#xD;&#xA;  [2]: http://datascience.stackexchange.com/questions/1119/predictive-modeling-based-on-rfm-scoring-indicators\" />  <row Id=\"8293\" PostHistoryTypeId=\"4\" PostId=\"5021\" RevisionGUID=\"cdf82280-41cb-421e-b7e2-17409c3c40bb\" CreationDate=\"2015-02-03T23:58:36.380\" UserId=\"97\" Comment=\"Retagging to relevant terms\" Text=\"Rank players of any given sport\" />  <row Id=\"8294\" PostHistoryTypeId=\"6\" PostId=\"5021\" RevisionGUID=\"cdf82280-41cb-421e-b7e2-17409c3c40bb\" CreationDate=\"2015-02-03T23:58:36.380\" UserId=\"97\" Comment=\"Retagging to relevant terms\" Text=\"&lt;predictive-modeling&gt;&lt;scoring&gt;&lt;ranking&gt;&lt;sports&gt;\" />  <row Id=\"8322\" PostHistoryTypeId=\"6\" PostId=\"4979\" RevisionGUID=\"e35f5092-810d-4c94-924c-596fd47e1698\" CreationDate=\"2015-02-04T15:46:20.233\" UserId=\"97\" Comment=\"Add new relevant tags\" Text=\"&lt;markov-process&gt;&lt;matlab&gt;&lt;simulation&gt;\" />  <row Id=\"8493\" PostHistoryTypeId=\"6\" PostId=\"5109\" RevisionGUID=\"e00a1214-829c-4b15-b551-2b24f4e0001a\" CreationDate=\"2015-02-11T16:12:47.457\" UserId=\"97\" Comment=\"additional tag\" Text=\"&lt;machine-learning&gt;&lt;bigdata&gt;&lt;finance&gt;\" />  <row Id=\"8664\" PostHistoryTypeId=\"2\" PostId=\"5175\" RevisionGUID=\"4f89a81b-44ae-4544-befb-90d6bbac08c4\" CreationDate=\"2015-02-18T10:03:13.913\" UserId=\"97\" Text=\"I often get the problem when this or that alias name is already used somewhere, and I can\\'t easily find that variable or aggregation to &#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/kK9mM.png&#xD;&#xA;&#xD;&#xA;Is there some place in Tableau where I can view/edit/reset full list of aliases?\" />  <row Id=\"8665\" PostHistoryTypeId=\"1\" PostId=\"5175\" RevisionGUID=\"4f89a81b-44ae-4544-befb-90d6bbac08c4\" CreationDate=\"2015-02-18T10:03:13.913\" UserId=\"97\" Text=\"List of used aliases in Tableau\" />  <row Id=\"8666\" PostHistoryTypeId=\"3\" PostId=\"5175\" RevisionGUID=\"4f89a81b-44ae-4544-befb-90d6bbac08c4\" CreationDate=\"2015-02-18T10:03:13.913\" UserId=\"97\" Text=\"&lt;visualization&gt;&lt;tools&gt;&lt;tableau&gt;\" />  <row Id=\"8684\" PostHistoryTypeId=\"5\" PostId=\"5175\" RevisionGUID=\"e0edd481-f815-45e7-b811-191d3fefb3fe\" CreationDate=\"2015-02-18T13:07:25.567\" UserId=\"97\" Comment=\"cosmetic changes\" Text=\"I often get the problem when this or that alias name is already used somewhere, and I can\\'t easily find that variable or aggregation to release the name.&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/kK9mM.png&#xD;&#xA;&#xD;&#xA;Is there some place in Tableau where I can view/edit/reset full list of aliases?\" />  <row Id=\"8707\" PostHistoryTypeId=\"2\" PostId=\"5184\" RevisionGUID=\"d56b6195-8566-4e6e-b3a3-00743d226686\" CreationDate=\"2015-02-19T12:06:08.173\" UserId=\"97\" Text=\"Finally solved the issue.&#xD;&#xA;You should go to:&#xD;&#xA;&#xD;&#xA;    Data –&gt; your_data_source –&gt; Edit Aliases –&gt; Measure Names&#xD;&#xA;&#xD;&#xA;And see full mapping between variables/aggregations and aliases.&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/uSoHC.png\" />  <row Id=\"9147\" PostHistoryTypeId=\"2\" PostId=\"5345\" RevisionGUID=\"36626ab1-cdae-4152-87df-a81fa16c6c32\" CreationDate=\"2015-03-18T11:39:40.353\" UserId=\"97\" Text=\"Has anyone used IDEA for R programming?&#xD;&#xA;I think of trying it. Products from JetBrains are usually outstanding.&#xD;&#xA;&#xD;&#xA;Or any development environments different from RStudio?\" />  <row Id=\"9148\" PostHistoryTypeId=\"1\" PostId=\"5345\" RevisionGUID=\"36626ab1-cdae-4152-87df-a81fa16c6c32\" CreationDate=\"2015-03-18T11:39:40.353\" UserId=\"97\" Text=\"IntelliJ IDEA for R programming\" />  <row Id=\"9149\" PostHistoryTypeId=\"3\" PostId=\"5345\" RevisionGUID=\"36626ab1-cdae-4152-87df-a81fa16c6c32\" CreationDate=\"2015-03-18T11:39:40.353\" UserId=\"97\" Text=\"&lt;r&gt;&lt;tools&gt;&lt;rstudio&gt;&lt;idea&gt;&lt;programming&gt;\" />  <row Id=\"9163\" PostHistoryTypeId=\"5\" PostId=\"5345\" RevisionGUID=\"b2fd4a23-1515-443c-af4b-043563994a6a\" CreationDate=\"2015-03-19T09:23:16.270\" UserId=\"97\" Comment=\"Retagging, update question\" Text=\"Has anyone used IDEA for R programming?&#xD;&#xA;I think of trying it. Products from JetBrains are usually outstanding.&#xD;&#xA;&#xD;&#xA;Or any development environments different from RStudio?&#xD;&#xA;&#xD;&#xA;**Update:**&#xD;&#xA;Renamed question. Lets\\'s discuss IDE alternatives to RStudio.\" />  <row Id=\"9164\" PostHistoryTypeId=\"4\" PostId=\"5345\" RevisionGUID=\"b2fd4a23-1515-443c-af4b-043563994a6a\" CreationDate=\"2015-03-19T09:23:16.270\" UserId=\"97\" Comment=\"Retagging, update question\" Text=\"IDE alternatives for R programming (RStudio, IntelliJ IDEA, Eclipse, Visual Studio)\" />  <row Id=\"9165\" PostHistoryTypeId=\"6\" PostId=\"5345\" RevisionGUID=\"b2fd4a23-1515-443c-af4b-043563994a6a\" CreationDate=\"2015-03-19T09:23:16.270\" UserId=\"97\" Comment=\"Retagging, update question\" Text=\"&lt;r&gt;&lt;tools&gt;&lt;rstudio&gt;&lt;programming&gt;\" />  <row Id=\"9205\" PostHistoryTypeId=\"5\" PostId=\"5345\" RevisionGUID=\"b359f4e2-f4d4-4cae-8fe5-4e8665fa6092\" CreationDate=\"2015-03-22T09:35:20.170\" UserId=\"97\" Comment=\"cosmetic changes\" Text=\"Has anyone used IntelliJ IDEA for R programming?&#xD;&#xA;Or any development environments different from RStudio?&#xD;&#xA;&#xD;&#xA;\" />  <row Id=\"9253\" PostHistoryTypeId=\"5\" PostId=\"5345\" RevisionGUID=\"e9be34b6-2a9e-4bef-8b43-b1bf51050b1e\" CreationDate=\"2015-03-25T09:29:10.363\" UserId=\"97\" Comment=\"concretizing question\" Text=\"I use RStudio for R programming. What majority does as well.&#xD;&#xA;And I guess it is absolute leader, but still remember about solid IDE-s from other technoly stacks, like Visual Studio or Eclipse.&#xD;&#xA;&#xD;&#xA;The questions are:&#xD;&#xA;&#xD;&#xA; 1. Please describe your experience of using different IDE-s, if there is any.&#xD;&#xA; 2. Does any of them have advantages over RStudio?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;\" />  <row Id=\"9254\" PostHistoryTypeId=\"5\" PostId=\"5345\" RevisionGUID=\"da09e67d-58ec-4e8a-9c62-b2f4baffc993\" CreationDate=\"2015-03-25T09:42:15.850\" UserId=\"97\" Comment=\"cosmetic change\" Text=\"I use RStudio for R programming. What majority does as well.&#xD;&#xA;And I guess it is absolute leader, but still remember about solid IDE-s from other technoly stacks, like Visual Studio or Eclipse.&#xD;&#xA;&#xD;&#xA;The questions are:&#xD;&#xA;&#xD;&#xA; 1. Please describe your experience of using IDE-s different from RStudio, if there is any.&#xD;&#xA; 2. Does any of them have advantages over RStudio?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;\" />  <row Id=\"9255\" PostHistoryTypeId=\"5\" PostId=\"5345\" RevisionGUID=\"348485ac-998b-411a-9bbb-b9e9af3265d3\" CreationDate=\"2015-03-25T09:49:10.830\" UserId=\"97\" Comment=\"cosmetic change\" Text=\"I use RStudio for R programming. What majority does as well.&#xD;&#xA;And I guess it is absolute leader, but still remember about solid IDE-s from other technoly stacks, like Visual Studio or Eclipse.&#xD;&#xA;&#xD;&#xA;The questions are:&#xD;&#xA;&#xD;&#xA; 1. Please describe your experience of using IDE-s different from RStudio, if there is any.&#xD;&#xA; 2. Does any of them have advantages over RStudio?&#xD;&#xA;&#xD;&#xA;I mostly mean debug/build/deploy features, besides coding itself, so text editors are probably not a solution.&#xD;&#xA;&#xD;&#xA;\" />  <row Id=\"9292\" PostHistoryTypeId=\"2\" PostId=\"5403\" RevisionGUID=\"61c02b69-608f-4182-a868-8e31e1e37c05\" CreationDate=\"2015-03-27T08:02:29.943\" UserId=\"97\" Text=\"What you are asking for is usually called **basket analysis**.&#xD;&#xA;&#xD;&#xA;I think you should get maximum value from the data you have by using both of them: user &amp; items.&#xD;&#xA;&#xD;&#xA;What you said about &quot;item-based&quot; approach means something like recommendation based on item tags or categories. It isn\\'t a recommendation system in the full sense, because it uses **your** categorization/tagging. In other words, you will never place diapers and beer into a single category, but it is still legendary buying pattern :). &#xD;&#xA;&#xD;&#xA;**Item-based** data is used typically in basket analysis algorithms, such as frequent pattern mining. In couple words: you search for most frequent item sets (items bought together or coherently) and make suggestions based on them.&#xD;&#xA;&#xD;&#xA;**User-based** data shouldn\\'t be ignored either. Clustering approach works here: you can find some groups of customers which buying attitude to item sets (found above) is different from the average.&#xD;&#xA;&#xD;&#xA;You can find more at statistics stack exchage site. &#xD;&#xA;Questions like **[this][1]**.&#xD;&#xA;And **[more][2]**.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stats.stackexchange.com/questions/95351/what-is-the-best-way-to-analyze-additional-attributes-in-a-market-basket-analysi&#xD;&#xA;  [2]: http://stats.stackexchange.com/search?q=basket%20analysis\" />  <row Id=\"9305\" PostHistoryTypeId=\"4\" PostId=\"5383\" RevisionGUID=\"a9fa2648-d10a-42f7-a89d-48ea285e77a2\" CreationDate=\"2015-03-27T11:22:03.790\" UserId=\"97\" Comment=\"Name and tags\" Text=\"Recommendation - item based vs user based\" />  <row Id=\"9306\" PostHistoryTypeId=\"6\" PostId=\"5383\" RevisionGUID=\"a9fa2648-d10a-42f7-a89d-48ea285e77a2\" CreationDate=\"2015-03-27T11:22:03.790\" UserId=\"97\" Comment=\"Name and tags\" Text=\"&lt;recommender-system&gt;&lt;beginner&gt;\" />  <row Id=\"9865\" PostHistoryTypeId=\"2\" PostId=\"5591\" RevisionGUID=\"dd17fa92-9ea5-447f-a030-f4d1f4850279\" CreationDate=\"2015-04-23T07:34:34.667\" UserId=\"97\" Text=\"There is pretty much mess in terminology in your question :).&#xD;&#xA;**[Data Regularization][1]** is used for model selection, it is not about data processing. [Here][2] it is described in more friendly manner.&#xD;&#xA;&#xD;&#xA;What you mean is **[Feature Scaling][3]**. It can be done in **several** ways including **[Rescaling][4]**, the method you described.&#xD;&#xA;&#xD;&#xA;[This][5] also might be helpful.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Regularization_(mathematics)&#xD;&#xA;  [2]: http://stats.stackexchange.com/questions/4961/what-is-regularization-in-plain-english&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Feature_scaling&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/Feature_scaling#Rescaling&#xD;&#xA;  [5]: http://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization\" />  <row Id=\"9866\" PostHistoryTypeId=\"5\" PostId=\"5591\" RevisionGUID=\"c5d945ac-f73c-4f6d-aee1-21fa5897f551\" CreationDate=\"2015-04-23T07:42:54.887\" UserId=\"97\" Comment=\"Added some links\" Text=\"There is pretty much mess in terminology in your question :).&#xD;&#xA;**[Data Regularization][1]** is used for model selection, it is not about data processing. [Here][2] it is described in more friendly manner.&#xD;&#xA;&#xD;&#xA;What you mean is **[Feature Scaling][3]**. It can be done in **several** ways including **[Rescaling][4]**, the method you described. You may also use Standardization (normalization) and Scaling to unit length.&#xD;&#xA;&#xD;&#xA;These answers may be helpful:&#xD;&#xA;&#xD;&#xA;- [Normalization vs Scaling][5]&#xD;&#xA;- [Normalization vs Standardization][6]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Regularization_(mathematics)&#xD;&#xA;  [2]: http://stats.stackexchange.com/questions/4961/what-is-regularization-in-plain-english&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Feature_scaling&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/Feature_scaling#Rescaling&#xD;&#xA;  [5]: http://stats.stackexchange.com/questions/35591/normalization-vs-scaling&#xD;&#xA;  [6]: http://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization\" />  <row Id=\"9867\" PostHistoryTypeId=\"4\" PostId=\"5590\" RevisionGUID=\"b9d9df4a-41ef-42e5-b764-578afc345463\" CreationDate=\"2015-04-23T07:44:25.927\" UserId=\"97\" Comment=\"Retagged + corrected some spelling mistakes\" Text=\"What is the best way to scale a numerical dataset\" />  <row Id=\"9868\" PostHistoryTypeId=\"6\" PostId=\"5590\" RevisionGUID=\"b9d9df4a-41ef-42e5-b764-578afc345463\" CreationDate=\"2015-04-23T07:44:25.927\" UserId=\"97\" Comment=\"Retagged + corrected some spelling mistakes\" Text=\"&lt;data-mining&gt;&lt;dataset&gt;&lt;beginner&gt;&lt;feature-scaling&gt;\" />  <row Id=\"10164\" PostHistoryTypeId=\"2\" PostId=\"5691\" RevisionGUID=\"2a537931-1e17-45a9-ae28-b2f03ce3d3bd\" CreationDate=\"2015-05-05T10:32:10.873\" UserId=\"97\" Text=\"Calcualte distance?&#xD;&#xA;&#xD;&#xA;Distance = data2 - data1 = [(1,**0**), (2,**0**), (3, **7**), (4, **7**), (5, **7**), (6, **0**)]\" />  <row Id=\"10165\" PostHistoryTypeId=\"5\" PostId=\"5691\" RevisionGUID=\"1af6e5af-0128-47d9-9c68-f6e4420f5109\" CreationDate=\"2015-05-05T10:46:09.693\" UserId=\"97\" Comment=\"Add wave defin.\" Text=\"Calcualte distance?&#xD;&#xA;&#xD;&#xA;Distance = data2 - data1 = [(1,**0**), (2,**0**), *(3, **7**), (4, **7**), (5, **7**)*, (6, **0**)]&#xD;&#xA;&#xD;&#xA;And what will you treat as *the same wave*?&#xD;&#xA;&#xD;&#xA;Any three (or more) nearby points that have the same distance... \" />  <row Id=\"10599\" PostHistoryTypeId=\"2\" PostId=\"5832\" RevisionGUID=\"812bbc7f-9b6a-49a9-8071-b009440643b3\" CreationDate=\"2015-05-19T09:29:02.810\" UserId=\"97\" Text=\"Data science is a multi discipline subject.&#xD;&#xA;In addition to math you need some programming skills and ideally - understanding of the domain where you are going to solve problems.&#xD;&#xA;&#xD;&#xA;You can start from online-courses and some introduction literature.&#xD;&#xA;Look at the answers at current site:&#xD;&#xA;&#xD;&#xA; - [Data Science Sertifications][2]&#xD;&#xA; - [Books about Data Science][3]&#xD;&#xA; - [Data Science vs Data Mining][4]&#xD;&#xA; - [Data Science is a trend or a long term concept][5]&#xD;&#xA; - [Ph.D. in Data Science?][6]&#xD;&#xA; - [Statistics + Computer Science = Data Science?][7]&#xD;&#xA; - [Starting career as Data Scientist, is Software Engineering experience required?][8]&#xD;&#xA; - [Career switch to Big Data Analytics][9]&#xD;&#xA; - [And more about career...][10]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The typical DS isslustration:&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/wpkyD.png&#xD;&#xA;  [2]: http://datascience.stackexchange.com/questions/334/what-do-you-think-of-data-science-certifications&#xD;&#xA;  [3]: http://datascience.stackexchange.com/questions/313/books-about-the-science-in-data-science&#xD;&#xA;  [4]: http://datascience.stackexchange.com/questions/14/is-data-science-the-same-as-data-mining&#xD;&#xA;  [5]: http://datascience.stackexchange.com/questions/159/is-data-science-just-a-trend-or-is-a-long-term-concept&#xD;&#xA;  [6]: http://datascience.stackexchange.com/questions/234/data-science-ph-d-program-what-do-you-think&#xD;&#xA;  [7]: http://datascience.stackexchange.com/questions/808/statistics-computer-science-data-science&#xD;&#xA;  [8]: http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require&#xD;&#xA;  [9]: http://datascience.stackexchange.com/questions/1216/career-switch-to-big-data-analytics&#xD;&#xA;  [10]: http://datascience.stackexchange.com/questions/tagged/career\" />  <row Id=\"10600\" PostHistoryTypeId=\"5\" PostId=\"5832\" RevisionGUID=\"3b1319ba-5bfb-471f-8796-2e20abe2819a\" CreationDate=\"2015-05-19T09:36:52.823\" UserId=\"97\" Comment=\"cosmetic text change\" Text=\"Data science is a multi discipline subject.&#xD;&#xA;In addition to math you need some programming skills and ideally - understanding of the domain where you are going to solve problems.&#xD;&#xA;&#xD;&#xA;You can start from online-courses and some introduction literature.&#xD;&#xA;Look at the answers at current site:&#xD;&#xA;&#xD;&#xA; - [Data Science Sertifications][2]&#xD;&#xA; - [Books about Data Science][3]&#xD;&#xA; - [Data Science vs Data Mining][4]&#xD;&#xA; - [Data Science is a trend or a long term concept][5]&#xD;&#xA; - [Ph.D. in Data Science?][6]&#xD;&#xA; - [Statistics + Computer Science = Data Science?][7]&#xD;&#xA; - [Starting career as Data Scientist, is Software Engineering experience required?][8]&#xD;&#xA; - [Career switch to Big Data Analytics][9]&#xD;&#xA; - [And more about career...][10]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I can\\'t resist from placing this illustration for the millionth time:&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/wpkyD.png&#xD;&#xA;  [2]: http://datascience.stackexchange.com/questions/334/what-do-you-think-of-data-science-certifications&#xD;&#xA;  [3]: http://datascience.stackexchange.com/questions/313/books-about-the-science-in-data-science&#xD;&#xA;  [4]: http://datascience.stackexchange.com/questions/14/is-data-science-the-same-as-data-mining&#xD;&#xA;  [5]: http://datascience.stackexchange.com/questions/159/is-data-science-just-a-trend-or-is-a-long-term-concept&#xD;&#xA;  [6]: http://datascience.stackexchange.com/questions/234/data-science-ph-d-program-what-do-you-think&#xD;&#xA;  [7]: http://datascience.stackexchange.com/questions/808/statistics-computer-science-data-science&#xD;&#xA;  [8]: http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require&#xD;&#xA;  [9]: http://datascience.stackexchange.com/questions/1216/career-switch-to-big-data-analytics&#xD;&#xA;  [10]: http://datascience.stackexchange.com/questions/tagged/career\" />  <row Id=\"10601\" PostHistoryTypeId=\"4\" PostId=\"5826\" RevisionGUID=\"cc2f99d9-b837-4aa2-acb4-3ea0bf01d22a\" CreationDate=\"2015-05-19T13:02:40.477\" UserId=\"97\" Comment=\"relevant name + retag\" Text=\"Master degree in Data Science\" />  <row Id=\"10602\" PostHistoryTypeId=\"6\" PostId=\"5826\" RevisionGUID=\"cc2f99d9-b837-4aa2-acb4-3ea0bf01d22a\" CreationDate=\"2015-05-19T13:02:40.477\" UserId=\"97\" Comment=\"relevant name + retag\" Text=\"&lt;education&gt;&lt;career&gt;\" />  <row Id=\"10691\" PostHistoryTypeId=\"2\" PostId=\"5861\" RevisionGUID=\"bcfaa33b-fafb-4f8a-9c2d-6eaef0eb4c8a\" CreationDate=\"2015-05-22T09:22:12.163\" UserId=\"97\" Text=\"Data science is much more than ML.&#xD;&#xA;It starts from simple data visuzlization and descriptive statistics.&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://pbs.twimg.com/media/B6sZGVfIQAAMZvH.png:large\" />  <row Id=\"10692\" PostHistoryTypeId=\"5\" PostId=\"5861\" RevisionGUID=\"4322f277-8d55-4e8b-96ab-b5503ed2feef\" CreationDate=\"2015-05-22T09:31:24.800\" UserId=\"97\" Comment=\"more info\" Text=\"Data science is much broader concept than machine learning.&#xD;&#xA;It starts from simple data visualization and descriptive statistics to get insights, manipulations like cleansing to prepare data. Before you can use some ML algorithms. &#xD;&#xA;&#xD;&#xA;Basically such huge stacks as bigdata, visualization and data preprocessing are out of machine learning scope. And they are all integral parts of &quot;Data Science&quot;.&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://pbs.twimg.com/media/B6sZGVfIQAAMZvH.png:large\" />  <row Id=\"10702\" PostHistoryTypeId=\"6\" PostId=\"5856\" RevisionGUID=\"859c8578-8ca1-493e-a11c-fc8beb64b5e4\" CreationDate=\"2015-05-22T19:35:35.173\" UserId=\"97\" Comment=\"more relevant tags\" Text=\"&lt;machine-learning&gt;&lt;definitions&gt;&lt;knowledge-base&gt;\" />  <row Id=\"10792\" PostHistoryTypeId=\"5\" PostId=\"5861\" RevisionGUID=\"18388bf2-c281-4fad-99eb-70238146a9b5\" CreationDate=\"2015-05-24T14:33:02.237\" UserId=\"97\" Comment=\"add link to image\" Text=\"Data science is much broader concept than machine learning.&#xD;&#xA;It starts from simple data visualization and descriptive statistics to get insights, manipulations like cleansing to prepare data. Before you can use some ML algorithms. &#xD;&#xA;&#xD;&#xA;Basically such huge stacks as bigdata, visualization and data preprocessing are out of machine learning scope. And they are all integral parts of &quot;Data Science&quot;.&#xD;&#xA;&#xD;&#xA;![][1]&#xD;&#xA;&#xD;&#xA;Large resolution image:&#xD;&#xA;https://whatsthebigdata.files.wordpress.com/2013/07/datascientistmap.png&#xD;&#xA;&#xD;&#xA;  [1]: https://pbs.twimg.com/media/B6sZGVfIQAAMZvH.png:large\" />  <row Id=\"10915\" PostHistoryTypeId=\"6\" PostId=\"5860\" RevisionGUID=\"278a73b8-7906-4994-a385-6a47d00d3b68\" CreationDate=\"2015-05-27T14:25:34.353\" UserId=\"97\" Comment=\"this Q is not about recommender system\" Text=\"&lt;beginner&gt;&lt;books&gt;\" />  <row Id=\"11553\" PostHistoryTypeId=\"5\" PostId=\"5345\" RevisionGUID=\"7c1f5d75-c847-4cbb-8fb2-65cdb41de05d\" CreationDate=\"2015-06-15T15:34:00.147\" UserId=\"97\" Comment=\"I\\'ve got a &quot;popular question&quot; badge for this question: it is interesting for community. And should be reopened.\" Text=\"I use RStudio for R programming. What majority does as well.&#xD;&#xA;And I guess it is absolute leader, but still remember about solid IDE-s from other technoly stacks, like Visual Studio or Eclipse.&#xD;&#xA;&#xD;&#xA;The questions are:&#xD;&#xA;&#xD;&#xA; 1. Please describe your experience of using IDE-s different from RStudio, if there is any.&#xD;&#xA; 2. Does any of them have advantages over RStudio?&#xD;&#xA;&#xD;&#xA;I mostly mean debug/build/deploy features, besides coding itself: so text editors are probably not a solution.&#xD;&#xA;&#xD;&#xA;\" />  <row Id=\"11565\" PostHistoryTypeId=\"5\" PostId=\"5345\" RevisionGUID=\"12bb48c8-383c-4dab-a4ea-f52fd170754c\" CreationDate=\"2015-06-16T07:51:34.547\" UserId=\"97\" Comment=\"fixed grammar, made question more focused \" Text=\"I use RStudio for R programming. I remember about solid IDE-s from other technology stacks, like Visual Studio or Eclipse.&#xD;&#xA;&#xD;&#xA;I have two questions:&#xD;&#xA;&#xD;&#xA; 1. What other IDE-s than RStudio are used (please consider providing some brief description on them).&#xD;&#xA; 2. Does any of them have noticeable advantages over RStudio?&#xD;&#xA;&#xD;&#xA;I mostly mean debug/build/deploy features, besides coding itself (so text editors are probably not a solution).&#xD;&#xA;&#xD;&#xA;\" />  <row Id=\"11990\" PostHistoryTypeId=\"2\" PostId=\"6275\" RevisionGUID=\"c110c643-2f2d-427d-822b-9a592d78bf78\" CreationDate=\"2015-06-30T06:44:20.367\" UserId=\"97\" Text=\"There are plenty of methods, variations of k-means for the case of mixed dataset: k-modes, k-protoypes etc.&#xD;&#xA;&#xD;&#xA;**[It has been discussed already.][1]**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data\" />  <row Id=\"11991\" PostHistoryTypeId=\"6\" PostId=\"6274\" RevisionGUID=\"c43b2d15-e3d4-4c29-b115-5d63ceafc543\" CreationDate=\"2015-06-30T06:45:58.747\" UserId=\"97\" Comment=\"edited tags\" Text=\"&lt;clustering&gt;&lt;k-means&gt;&lt;categorical-data&gt;\" />  <row Id=\"12778\" PostHistoryTypeId=\"4\" PostId=\"1110\" RevisionGUID=\"66a5d1f0-87cf-477b-ae83-7e4f5b65ae94\" CreationDate=\"2015-07-22T12:31:37.033\" UserId=\"97\" Comment=\"Rename and add Binning tag\" Text=\"Binning long-tailed / pareto data before clustering\" />  <row Id=\"12779\" PostHistoryTypeId=\"6\" PostId=\"1110\" RevisionGUID=\"66a5d1f0-87cf-477b-ae83-7e4f5b65ae94\" CreationDate=\"2015-07-22T12:31:37.033\" UserId=\"97\" Comment=\"Rename and add Binning tag\" Text=\"&lt;clustering&gt;&lt;k-means&gt;\" />  <row Id=\"12780\" PostHistoryTypeId=\"6\" PostId=\"587\" RevisionGUID=\"37086bd3-7d61-4e6e-bee2-4e6471bf6f5d\" CreationDate=\"2015-07-22T12:34:31.783\" UserId=\"97\" Comment=\"add market data tag\" Text=\"&lt;dataset&gt;\" />  <row Id=\"15601\" PostHistoryTypeId=\"6\" PostId=\"8161\" RevisionGUID=\"1768b566-b60b-40b4-ad10-527b01b77806\" CreationDate=\"2015-09-23T09:14:38.457\" UserId=\"97\" Comment=\"Adding right tags.\" Text=\"&lt;predictive-modeling&gt;&lt;linear-regression&gt;&lt;scoring&gt;\" />  <row Id=\"15615\" PostHistoryTypeId=\"2\" PostId=\"8170\" RevisionGUID=\"853db77f-0a44-489d-a625-4db226d3d149\" CreationDate=\"2015-09-23T11:34:31.400\" UserId=\"97\" Text=\"Predicting and scoring are two different tasks.&#xD;&#xA;&#xD;&#xA;And according to your answers and comments you are not solving prediction problem. You just want to set to each student a number in range [1,100] according to some rule. This is **ranking** (or scoring, whatever).&#xD;&#xA;&#xD;&#xA;Therefore, the terms #prediction_model, #accuracy, #validation, #training_set are **out of this scope**. You don\\'t need to validate anything. You are not making predictions.&#xD;&#xA;&#xD;&#xA;What yo want is to map ranks to students.&#xD;&#xA;A trivial task.&#xD;&#xA;&#xD;&#xA; 1. Normalize data: each of your factors&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA; 2. Multiple by 100, as you want [0,100] range instead of [0,1]&#xD;&#xA; 3. Set up weights based on your experience according to factor\\'s importance. So that the sum of weights is 1.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/cdJkC.png\" />  <row Id=\"15616\" PostHistoryTypeId=\"12\" PostId=\"8170\" RevisionGUID=\"cbe7b4c2-170b-45cc-9511-409c8481158c\" CreationDate=\"2015-09-23T11:36:19.290\" UserId=\"97\" Comment=\"via Vote\" Text=\"{&quot;Voters&quot;:[{&quot;Id&quot;:97,&quot;DisplayName&quot;:&quot;IharS&quot;}]}\" />  <row Id=\"15618\" PostHistoryTypeId=\"5\" PostId=\"8170\" RevisionGUID=\"cc9aa156-0f3b-4feb-b93c-d5cbd7bbf4dd\" CreationDate=\"2015-09-23T11:41:22.810\" UserId=\"97\" Comment=\"added 236 characters in body\" Text=\"Predicting and scoring are two different tasks.&#xD;&#xA;&#xD;&#xA;And according to your answers and comments you are not solving prediction problem. You just want to set to each student a number in range [1,100] according to some rule. This is **ranking** (or scoring, whatever).&#xD;&#xA;&#xD;&#xA;Therefore, the terms #prediction_model, #accuracy, #validation, #training_set are **out of this scope**. You don\\'t need to validate anything. You are not making predictions.&#xD;&#xA;&#xD;&#xA;What yo want is to map ranks to students.&#xD;&#xA;&#xD;&#xA;But a problem is that you have mostely categorical data (school name, location etc) that cannot be \\'ranked\\'.&#xD;&#xA;&#xD;&#xA;If you change it somehow to numerical (e.g. \\'Skill_1_level\\', \\'Skill_2_level\\', \\'remoteness_of_location\\' etc) than you can do some ranking.&#xD;&#xA;&#xD;&#xA; 1. Normalize data: each of your factors&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA; 2. Multiple by 100, as you want [0,100] range instead of [0,1]&#xD;&#xA; 3. Set up weights based on your experience according to factor\\'s importance. So that the sum of weights is 1.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/cdJkC.png\" />  <row Id=\"15619\" PostHistoryTypeId=\"13\" PostId=\"8170\" RevisionGUID=\"4b7961d3-4371-4990-bfea-570818a87aac\" CreationDate=\"2015-09-23T11:41:26.683\" UserId=\"97\" Text=\"{&quot;Voters&quot;:[{&quot;Id&quot;:97,&quot;DisplayName&quot;:&quot;IharS&quot;}]}\" />  <row Id=\"15620\" PostHistoryTypeId=\"5\" PostId=\"8170\" RevisionGUID=\"2d9f9fde-df7b-4835-b65b-1344f1cd1f61\" CreationDate=\"2015-09-23T11:46:54.070\" UserId=\"97\" Comment=\"added 236 characters in body\" Text=\"Predicting and scoring are two different tasks.&#xD;&#xA;&#xD;&#xA;And according to your answers and comments you are not solving prediction problem. You just want to set to each student a number in range [1,100] according to some rule. This is **ranking** (or scoring, whatever).&#xD;&#xA;&#xD;&#xA;Therefore, the terms #prediction_model, #accuracy, #validation, #training_set are **out of this scope**. You don\\'t need to validate anything. You are not making predictions.&#xD;&#xA;&#xD;&#xA;What you want is to map ranks to students.&#xD;&#xA;&#xD;&#xA;But a problem is that you have mostely categorical data (school name, location etc) that cannot be \\'ranked\\'. Some of them are useless at all: how does the student name refer to his school progress? :)&#xD;&#xA;&#xD;&#xA;If you change it somehow to numerical (e.g. \\'Skill_1_level\\', \\'Skill_2_level\\', \\'remoteness_of_location\\', \\'school rank\\' etc) than you can do some ranking:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; 1. Normalize data: each of your factors&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA; 2. Multiple by 100, as you want [0,100] range instead of [0,1]&#xD;&#xA; 3. Set up weights based on your experience according to factor\\'s importance. So that the sum of weights is 1.&#xD;&#xA; 4. And finally build a rank (score):&#xD;&#xA;&#xD;&#xA;**Rank = 0.1 * skill_1_level + 0.2 * skill_2_level + 0.05 * remoteness_of_location + 0.5 * school_rank + ...**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/cdJkC.png\" />  <row Id=\"15621\" PostHistoryTypeId=\"6\" PostId=\"8161\" RevisionGUID=\"c4a4f01c-a4fa-4975-8f6f-6cd904785d74\" CreationDate=\"2015-09-23T11:49:55.610\" UserId=\"97\" Comment=\"this is not predictive modeling\" Text=\"&lt;dataset&gt;&lt;linear-regression&gt;&lt;scoring&gt;\" />  <row Id=\"15625\" PostHistoryTypeId=\"6\" PostId=\"8161\" RevisionGUID=\"c83fece6-2b97-475e-9c70-452bb42f5615\" CreationDate=\"2015-09-23T14:22:32.967\" UserId=\"97\" Comment=\"adding some tags from stats.SE\" Text=\"&lt;dataset&gt;&lt;categorical-data&gt;&lt;scoring&gt;&lt;normalization&gt;&lt;weighted-data&gt;\" />  <row Id=\"15748\" PostHistoryTypeId=\"2\" PostId=\"8212\" RevisionGUID=\"59447385-6dd3-4710-a977-4a98389978ec\" CreationDate=\"2015-09-26T10:30:28.030\" UserId=\"97\" Text=\"Looks like you have two problems: time-dependant predictive modeling and feature engineering.&#xD;&#xA;&#xD;&#xA;**1) Time-dependant data**&#xD;&#xA;&#xD;&#xA;Key words:&#xD;&#xA;&#xD;&#xA;&gt; create a model which should be able to predict **the future level** of a rider in professional cycling&#xD;&#xA;&#xD;&#xA;The *future* Level of the Rider.&#xD;&#xA;&#xD;&#xA;That means there is *current* Level. And Level *in the past*. And the history of Level change in time for each Rider.&#xD;&#xA;&#xD;&#xA;The problem you are trying to solve is time dependant. Your source data can look like:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**2) Build a variable to predict**&#xD;&#xA;&#xD;&#xA;Second. You should build the target feature. This Rider Level (Class).&#xD;&#xA;&#xD;&#xA;As far as I understood:&#xD;&#xA;&#xD;&#xA; - the overall rider \\'level\\' can be a function of his levels in each particular race types&#xD;&#xA; - and... the race \\'observation\\' class is a kind of function of `{Category, Race Type, Parcours, Distance, Position}`&#xD;&#xA;&#xD;&#xA;**So the only thing you realy need to predict is race result.**&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/eNQKD.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/TAwV6.png\" />  <row Id=\"15749\" PostHistoryTypeId=\"6\" PostId=\"8207\" RevisionGUID=\"7d48a4ed-3058-44ca-ac7c-673bbf6d963d\" CreationDate=\"2015-09-26T10:32:17.323\" UserId=\"97\" Comment=\"relevant tagging\" Text=\"&lt;classification&gt;&lt;predictive-modeling&gt;&lt;regression&gt;&lt;feature-selection&gt;&lt;feature-construction&gt;\" />  <row Id=\"15750\" PostHistoryTypeId=\"5\" PostId=\"8212\" RevisionGUID=\"33f53b9f-e044-402d-a339-91277a048452\" CreationDate=\"2015-09-26T10:44:53.643\" UserId=\"97\" Comment=\"added 3 characters in body\" Text=\"Looks like you have two problems: time-dependant predictive modeling and feature engineering.&#xD;&#xA;&#xD;&#xA;**1) Time-dependant data**&#xD;&#xA;&#xD;&#xA;Key words:&#xD;&#xA;&#xD;&#xA;&gt; create a model which should be able to predict **the future level** of a rider in professional cycling&#xD;&#xA;&#xD;&#xA;The *future* Level of the Rider.&#xD;&#xA;&#xD;&#xA;That means there is *current* Level. And Level *in the past*. And the history of Level change in time for each Rider.&#xD;&#xA;&#xD;&#xA;The problem you are trying to solve is time dependant. Your source data can look like:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**2) Build a variable to predict**&#xD;&#xA;&#xD;&#xA;Second. You should build the target feature. This Rider Level (Class).&#xD;&#xA;&#xD;&#xA;As far as I understood:&#xD;&#xA;&#xD;&#xA; - the overall rider \\'level\\' can be a function of his \\'classes\\' in each particular race types&#xD;&#xA; - and... the race \\'observation\\' class is a kind of function of `{Category, Race Type, Parcours, Distance, Position}`&#xD;&#xA;&#xD;&#xA;**So the only thing you realy need to predict is race result.**&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/eNQKD.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/TAwV6.png\" />  <row Id=\"15793\" PostHistoryTypeId=\"2\" PostId=\"8224\" RevisionGUID=\"9e8af460-2892-41d5-8c79-b787214f3411\" CreationDate=\"2015-09-28T21:15:42.943\" UserId=\"97\" Text=\"I guess you are not limited to these 100 samples. Generate more, and let each 5th be negative. Then reduce number of positives by random removing 4/5 of them.&#xD;&#xA;&#xD;&#xA;And check this out [Training imbalanced data set][1]&#xD;&#xA;&#xD;&#xA;This is small quantity, you\\'d better have 50:50 negative vs positive.&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets\" />  <row Id=\"15797\" PostHistoryTypeId=\"6\" PostId=\"6939\" RevisionGUID=\"bc070527-ab6b-4170-88ed-e44ebe8b8efa\" CreationDate=\"2015-09-28T21:17:48.647\" UserId=\"97\" Comment=\"relative tags added\" Text=\"&lt;classification&gt;&lt;dataset&gt;&lt;unbalanced-classes&gt;\" />  <row Id=\"15798\" PostHistoryTypeId=\"2\" PostId=\"8226\" RevisionGUID=\"b23bd797-357d-4f6f-ad33-19f607300da2\" CreationDate=\"2015-09-28T21:43:27.583\" UserId=\"97\" Text=\"This is supervised learning problem. `Type` is a predictor. `#serviced` classifier is target variable. Model is trained on samples set you already have. Best guess is that any model will not have substantual predictive ability.&#xD;&#xA;&#xD;&#xA;Try including more factors (predictors) into the model. Like `years_being_in_usage`, `equipment_model`, `have_been_in_service_before` and so on. The more you get, the better model you can train.\" />  <row Id=\"15799\" PostHistoryTypeId=\"6\" PostId=\"8225\" RevisionGUID=\"fa170eba-bb08-4c2d-8d63-efed474b86e6\" CreationDate=\"2015-09-28T21:44:48.190\" UserId=\"97\" Comment=\"relative tags\" Text=\"&lt;machine-learning&gt;&lt;classification&gt;&lt;supervised-learning&gt;\" />  <row Id=\"15800\" PostHistoryTypeId=\"6\" PostId=\"8225\" RevisionGUID=\"8a3174bf-0812-4f8a-a73b-cf264b409248\" CreationDate=\"2015-09-28T21:53:40.140\" UserId=\"97\" Comment=\"edited tags\" Text=\"&lt;machine-learning&gt;&lt;classification&gt;&lt;predictive-modeling&gt;&lt;supervised-learning&gt;\" />  <row Id=\"15834\" PostHistoryTypeId=\"2\" PostId=\"8237\" RevisionGUID=\"8e4c1fb3-138a-4d0c-9881-1deca6381515\" CreationDate=\"2015-09-29T21:32:45.653\" UserId=\"97\" Text=\"The first question is: what to do you want to see in user profile?&#xD;&#xA;&#xD;&#xA; - Top-10 tracks, top-10 artist by user?&#xD;&#xA; - How many tracks/artists a user listens to in a day on average (may be, in last month)?&#xD;&#xA;&#xD;&#xA;May be you want to get some general information related to the whole user base:&#xD;&#xA;&#xD;&#xA; - Which artist/track is the most popular among users from different countries (top-N of them)?&#xD;&#xA;&#xD;&#xA;The second: you store and want to make aggregates for millions of records. It\\'s not a text file deal. Make a database. Create a table `id-user | country | id-artist | id-track`. Create another tables with some aggregates from #1, update it regularly and diplay on the front end.\" />  <row Id=\"15835\" PostHistoryTypeId=\"4\" PostId=\"8236\" RevisionGUID=\"4f1139c3-ffba-47d9-b67f-47e9c2be624e\" CreationDate=\"2015-09-29T21:34:56.103\" UserId=\"97\" Comment=\"related tags\" Text=\"Create user profile in music web application\" />  <row Id=\"15836\" PostHistoryTypeId=\"6\" PostId=\"8236\" RevisionGUID=\"4f1139c3-ffba-47d9-b67f-47e9c2be624e\" CreationDate=\"2015-09-29T21:34:56.103\" UserId=\"97\" Comment=\"related tags\" Text=\"&lt;databases&gt;&lt;feature-construction&gt;&lt;descriptive-statistics&gt;&lt;aggregation&gt;\" />  <row Id=\"15837\" PostHistoryTypeId=\"6\" PostId=\"8236\" RevisionGUID=\"c6eb4fde-e05a-43b7-8559-22a7362c5b2e\" CreationDate=\"2015-09-29T21:45:42.390\" UserId=\"97\" Comment=\"edited tags\" Text=\"&lt;beginner&gt;&lt;databases&gt;&lt;feature-construction&gt;&lt;descriptive-statistics&gt;&lt;aggregation&gt;\" />  <row Id=\"15838\" PostHistoryTypeId=\"5\" PostId=\"8226\" RevisionGUID=\"998627db-e5fe-426b-a453-9d3efbd86699\" CreationDate=\"2015-09-29T21:48:29.687\" UserId=\"97\" Comment=\"added 22 characters in body\" Text=\"This is supervised learning problem. `Type` is a predictor. `#serviced` classifier is target variable. Model is trained on samples set you already have. Best guess is that any model will not have substantual predictive ability. `Type` is not enough.&#xD;&#xA;&#xD;&#xA;Try including more factors (predictors) into the model. Like `years_being_in_usage`, `equipment_model`, `have_been_in_service_before` and so on. The more you get, the better model you can train.\" />  <row Id=\"16137\" PostHistoryTypeId=\"6\" PostId=\"8322\" RevisionGUID=\"51f1cb64-d0d2-4d94-9b34-4f034c684ec3\" CreationDate=\"2015-10-08T11:25:06.710\" UserId=\"97\" Comment=\"relative tagging\" Text=\"&lt;data-mining&gt;&lt;missing-data&gt;\" />  <row Id=\"16213\" PostHistoryTypeId=\"4\" PostId=\"262\" RevisionGUID=\"8d40e60e-f010-4245-be3c-48d78e2cb2ae\" CreationDate=\"2015-10-10T12:30:56.583\" UserId=\"97\" Comment=\"edited title\" Text=\"Hierarchical Data Format. What are the advantages compared to alternative formats?\" />  <row Id=\"17051\" PostHistoryTypeId=\"6\" PostId=\"810\" RevisionGUID=\"6314a53c-9119-4799-abab-25bd77d5d4d2\" CreationDate=\"2015-10-28T09:22:22.083\" UserId=\"97\" Comment=\"more relevant tags\" Text=\"&lt;machine-learning&gt;&lt;dataset&gt;&lt;unbalanced-classes&gt;\" />  <row Id=\"17386\" PostHistoryTypeId=\"2\" PostId=\"8741\" RevisionGUID=\"54a636a6-035a-47af-a1c8-5623e8b50e0c\" CreationDate=\"2015-11-06T15:11:04.593\" UserId=\"97\" Text=\"This is something you can get in Tableau in 10 minutes.&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;What you need is year binning, skills weights and length(year_end - year_start) for Gannt chart:&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/HE5go.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/sTAAt.png\" />  <row Id=\"17387\" PostHistoryTypeId=\"6\" PostId=\"8688\" RevisionGUID=\"34feb9a0-35e8-4dc8-8d5a-996c17bcefe8\" CreationDate=\"2015-11-06T15:17:03.667\" UserId=\"97\" Comment=\"more useful tags\" Text=\"&lt;r&gt;&lt;visualization&gt;&lt;categorical-data&gt;&lt;tableau&gt;\" />  <row Id=\"19780\" PostHistoryTypeId=\"2\" PostId=\"9554\" RevisionGUID=\"2a2f318b-ceda-49ca-b2f3-5c5fa325ad38\" CreationDate=\"2015-12-30T09:37:40.547\" UserId=\"97\" Text=\"Contribution into open source projects is typically a good way to get some practise for newbies, and try a new area for experienced data scientists and analysts.&#xD;&#xA;&#xD;&#xA;Which projects do you contribute? Please provide some intro + link on Github.\" />  <row Id=\"19781\" PostHistoryTypeId=\"1\" PostId=\"9554\" RevisionGUID=\"2a2f318b-ceda-49ca-b2f3-5c5fa325ad38\" CreationDate=\"2015-12-30T09:37:40.547\" UserId=\"97\" Text=\"Open source data science projects to contribute\" />  <row Id=\"19782\" PostHistoryTypeId=\"3\" PostId=\"9554\" RevisionGUID=\"2a2f318b-ceda-49ca-b2f3-5c5fa325ad38\" CreationDate=\"2015-12-30T09:37:40.547\" UserId=\"97\" Text=\"&lt;open-source&gt;\" />  <row Id=\"20815\" PostHistoryTypeId=\"5\" PostId=\"201\" RevisionGUID=\"16155407-67b0-4dce-969a-fbf32091b62c\" CreationDate=\"2016-01-21T08:01:34.380\" UserId=\"97\" Comment=\"added one more data source\" Text=\"Update:&#xD;&#xA;---&#xD;&#xA;**Kaggle.com**, a home of modern data science &amp; machine learning enthusiasts, opened **[it\\'s own repository of the data sets][1]**.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;In addition to the listed sources.&#xD;&#xA;&#xD;&#xA;Some social network data sets:&#xD;&#xA;&#xD;&#xA; - [Stanford University large network dataset collection (SNAP)][2]&#xD;&#xA; - [A huge twitter dataset that includes followers][3] + [large collection of twitter datasets here][4]&#xD;&#xA; - [LastFM data set][5]&#xD;&#xA;&#xD;&#xA;There are plenty of sources listed at Stats SE:&#xD;&#xA;&#xD;&#xA; - [Locating freely available data samples][6]&#xD;&#xA; - [Data APIs/feeds available as packages in R][7]&#xD;&#xA; - [Free data set for very high dimensional classification][8]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.kaggle.com/datasets&#xD;&#xA;  [2]: http://snap.stanford.edu/data/&#xD;&#xA;  [3]: http://blog.infochimps.com/2008/12/29/massive-scrape-of-twitters-friend-graph/&#xD;&#xA;  [4]: http://www.infochimps.com/collections/twitter-census&#xD;&#xA;  [5]: http://mtg.upf.edu/node/1671&#xD;&#xA;  [6]: http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/&#xD;&#xA;  [7]: http://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r&#xD;&#xA;  [8]: http://stats.stackexchange.com/questions/973/free-data-set-for-very-high-dimensional-classification\" />  <row Id=\"20817\" PostHistoryTypeId=\"5\" PostId=\"201\" RevisionGUID=\"d0902eb9-86de-4f31-9ed4-956e1f15689b\" CreationDate=\"2016-01-21T08:44:01.610\" UserId=\"97\" Comment=\"deleted 3 characters in body\" Text=\"Update:&#xD;&#xA;&#xD;&#xA;**Kaggle.com**, a home of modern data science &amp; machine learning enthusiasts:), opened **[it\\'s own repository of the data sets][1]**.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;In addition to the listed sources.&#xD;&#xA;&#xD;&#xA;Some social network data sets:&#xD;&#xA;&#xD;&#xA; - [Stanford University large network dataset collection (SNAP)][2]&#xD;&#xA; - [A huge twitter dataset that includes followers][3] + [large collection of twitter datasets here][4]&#xD;&#xA; - [LastFM data set][5]&#xD;&#xA;&#xD;&#xA;There are plenty of sources listed at Stats SE:&#xD;&#xA;&#xD;&#xA; - [Locating freely available data samples][6]&#xD;&#xA; - [Data APIs/feeds available as packages in R][7]&#xD;&#xA; - [Free data set for very high dimensional classification][8]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.kaggle.com/datasets&#xD;&#xA;  [2]: http://snap.stanford.edu/data/&#xD;&#xA;  [3]: http://blog.infochimps.com/2008/12/29/massive-scrape-of-twitters-friend-graph/&#xD;&#xA;  [4]: http://www.infochimps.com/collections/twitter-census&#xD;&#xA;  [5]: http://mtg.upf.edu/node/1671&#xD;&#xA;  [6]: http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/&#xD;&#xA;  [7]: http://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r&#xD;&#xA;  [8]: http://stats.stackexchange.com/questions/973/free-data-set-for-very-high-dimensional-classification\" />'), ('118', '  <row Id=\"68\" PostHistoryTypeId=\"2\" PostId=\"28\" RevisionGUID=\"d398a744-6316-41a9-b3c6-1a6e509528da\" CreationDate=\"2014-05-14T07:55:40.133\" UserId=\"118\" Text=\"There is free ebook &quot;[Introduction to Data Science][1]&quot; based on [tag:R] language&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://jsresearch.net/\" />  <row Id=\"74\" PostHistoryTypeId=\"2\" PostId=\"31\" RevisionGUID=\"26fa9a8e-80d6-4965-9725-009a067b91fc\" CreationDate=\"2014-05-14T08:38:07.007\" UserId=\"118\" Text=\"I have a bunch of customer profiles stroed in [tag:elasticsearch] cluster. These profiles are now used for creatinon of target groups for our email subscriptions. &#xD;&#xA;&#xD;&#xA;Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).&#xD;&#xA;&#xD;&#xA;I wonder how can I search for interesting groups automatically - using datascience, machine learning, clustering or somewhat else.&#xD;&#xA;&#xD;&#xA;[tag:R] programming langugae seems to be a good tool for this task, but I can\\'t form a methodology of such group search. One solution is to find somehow largest clusters of customers and use them as target groups, so the question is:&#xD;&#xA;&#xD;&#xA;**How can I automatically choose largest clusters of similar customers (similar by parameters that I don\\'t know at this moment)?**&#xD;&#xA;&#xD;&#xA;For example: my program will connect elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.&#xD;&#xA;&#xD;&#xA;\" />  <row Id=\"75\" PostHistoryTypeId=\"1\" PostId=\"31\" RevisionGUID=\"26fa9a8e-80d6-4965-9725-009a067b91fc\" CreationDate=\"2014-05-14T08:38:07.007\" UserId=\"118\" Text=\"Clustering customer data stored in ElasticSearch\" />  <row Id=\"76\" PostHistoryTypeId=\"3\" PostId=\"31\" RevisionGUID=\"26fa9a8e-80d6-4965-9725-009a067b91fc\" CreationDate=\"2014-05-14T08:38:07.007\" UserId=\"118\" Text=\"&lt;data-mining&gt;&lt;clustering&gt;\" />  <row Id=\"81\" PostHistoryTypeId=\"5\" PostId=\"31\" RevisionGUID=\"d92b7e62-ad68-41e8-bf53-df02eaf2e2c7\" CreationDate=\"2014-05-14T09:07:44.440\" UserId=\"118\" Comment=\"fix typo\" Text=\"I have a bunch of customer profiles stroed in [tag:elasticsearch] cluster. These profiles are now used for creatinon of target groups for our email subscriptions. &#xD;&#xA;&#xD;&#xA;Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).&#xD;&#xA;&#xD;&#xA;I wonder how can I search for interesting groups automatically - using datascience, machine learning, clustering or somewhat else.&#xD;&#xA;&#xD;&#xA;[tag:R] programming language seems to be a good tool for this task, but I can\\'t form a methodology of such group search. One solution is to find somehow largest clusters of customers and use them as target groups, so the question is:&#xD;&#xA;&#xD;&#xA;**How can I automatically choose largest clusters of similar customers (similar by parameters that I don\\'t know at this moment)?**&#xD;&#xA;&#xD;&#xA;For example: my program will connect elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.&#xD;&#xA;&#xD;&#xA;\" />  <row Id=\"113\" PostHistoryTypeId=\"6\" PostId=\"41\" RevisionGUID=\"2a9da2b2-a929-4f6d-baf4-1f346da68021\" CreationDate=\"2014-05-14T13:06:28.407\" UserId=\"118\" Comment=\"added tag r\" Text=\"&lt;bigdata&gt;&lt;r&gt;\" />  <row Id=\"151\" PostHistoryTypeId=\"2\" PostId=\"60\" RevisionGUID=\"9d921d53-259d-4dcc-aebe-f8dacb8a8f29\" CreationDate=\"2014-05-14T17:58:48.297\" UserId=\"118\" Text=\"R performs all computation in-memory so you can\\'t perform operation on a dataset that is larger than available RAM amount. However there are some libraries that allow bigdata processing using R and one of popular libraries for bigdata processing like Hadoop.\" />  <row Id=\"223\" PostHistoryTypeId=\"6\" PostId=\"81\" RevisionGUID=\"33469ce8-85a3-473b-8e05-b629e511ba29\" CreationDate=\"2014-05-15T09:31:51.370\" UserId=\"118\" Comment=\"retagged question\" Text=\"&lt;definitions&gt;&lt;parallel&gt;&lt;distributed&gt;\" />  <row Id=\"236\" PostHistoryTypeId=\"6\" PostId=\"77\" RevisionGUID=\"b12d3a4f-33fc-4888-9bee-c94c051c1c46\" CreationDate=\"2014-05-15T13:15:02.727\" UserId=\"118\" Comment=\"added neo4j tag\" Text=\"&lt;databases&gt;&lt;nosql&gt;&lt;neo4j&gt;\" />  <row Id=\"284\" PostHistoryTypeId=\"5\" PostId=\"66\" RevisionGUID=\"3f29bc47-4f5c-4b70-bc50-6817e9323191\" CreationDate=\"2014-05-16T13:45:57.450\" UserId=\"118\" Comment=\"some info about tag\" Text=\"Big data is the term for a collection of data sets so large and complex that it becomes difficult to process using on-hand database management tools or traditional data processing applications. The challenges include capture, curation, storage, search, sharing, transfer, analysis and visualization.\" />  <row Id=\"370\" PostHistoryTypeId=\"2\" PostId=\"139\" RevisionGUID=\"3441d74f-46c8-4df4-a3ae-b7427bd72a75\" CreationDate=\"2014-05-18T14:04:37.870\" UserId=\"118\" Text=\"I\\'d suggest [Apache Kafka][1] as message store and any stream processing solution of your choice like [Apache Camel][2] or [Twitter Storm][3]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://kafka.apache.org&#xD;&#xA;  [2]: https://camel.apache.org&#xD;&#xA;  [3]: https://github.com/apache/incubator-storm\" />  <row Id=\"371\" PostHistoryTypeId=\"2\" PostId=\"140\" RevisionGUID=\"61a0191e-f112-4499-a474-3f023a96be0a\" CreationDate=\"2014-05-18T14:30:10.553\" UserId=\"118\" Text=\"I\\'ve read very good [article][1] recently that suggests using [Twitter storm][2] for a task that looks pretty similar to yours.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.michael-noll.com/blog/2013/01/18/implementing-real-time-trending-topics-in-storm/&#xD;&#xA;  [2]: https://github.com/nathanmarz/storm\" />  <row Id=\"388\" PostHistoryTypeId=\"6\" PostId=\"76\" RevisionGUID=\"2cfaeaf8-6e0b-4b19-b54c-6c847299f712\" CreationDate=\"2014-05-18T15:18:08.050\" UserId=\"118\" Comment=\"retagged post\" Text=\"&lt;bigdata&gt;&lt;tools&gt;&lt;data-stream-mining&gt;\" />  <row Id=\"412\" PostHistoryTypeId=\"2\" PostId=\"156\" RevisionGUID=\"f7a4357c-8d2f-4c77-b676-b707d58da289\" CreationDate=\"2014-05-18T19:19:44.240\" UserId=\"118\" Text=\"[Freebase][1] is a free community driven database that spans many interesting topics and contains about 2,5 billion facts in machine readable format. It is also have good API to perform data queries.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.freebase.com\" />  <row Id=\"429\" PostHistoryTypeId=\"6\" PostId=\"107\" RevisionGUID=\"0b5a80cf-c9d0-4ae4-b424-917a23944beb\" CreationDate=\"2014-05-19T07:33:50.080\" UserId=\"118\" Comment=\"retagged post\" Text=\"&lt;tools&gt;&lt;data-stream-mining&gt;\" />  <row Id=\"478\" PostHistoryTypeId=\"5\" PostId=\"147\" RevisionGUID=\"ef541769-5807-421c-a994-36f865339f34\" CreationDate=\"2014-05-20T13:52:59.427\" UserId=\"118\" Comment=\"added 449 characters in body\" Text=\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human–computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\" />  <row Id=\"486\" PostHistoryTypeId=\"5\" PostId=\"145\" RevisionGUID=\"ac8729f8-7707-4822-8e53-8b0ee6f752c6\" CreationDate=\"2014-05-20T13:53:26.907\" UserId=\"118\" Comment=\"added 448 characters in body\" Text=\"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval etc.\" />  <row Id=\"43519\" PostHistoryTypeId=\"5\" PostId=\"156\" RevisionGUID=\"9cd30d64-8da9-4181-8c05-98353258dc91\" CreationDate=\"2017-02-01T12:36:02.110\" UserId=\"118\" Comment=\"added 90 characters in body\" Text=\"[Freebase][1] is a free community driven database that spans many interesting topics and contains about 2,5 billion facts in machine readable format. It is also have good API to perform data queries.&#xD;&#xA;&#xD;&#xA;Here is another compiled list of open data sets: http://www.datapure.co/open-data-sets&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.freebase.com\" />')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Learning_Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "book = sc.textFile(\"./PostHistory.xml\")\n",
    "javaLines = book.filter(lambda line: \"UserId\" in line).map(lambda line: (line.split('\"')[11], line)).reduceByKey(lambda x,y:x+y)\n",
    "print(javaLines.take(5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PostHistory file, provide a total count of the words used by each distinct user. In other\n",
    "words, count all words in all posts for each user and display this to screen. You can only identify\n",
    "users by the UserID (30 points). You get 15 bonus points if you get the actual DisplayName of\n",
    "the user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
